{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1 Week 4  \n",
    "## Lecture 1. Maximum Expected Utility\n",
    "### 1.1 Simple Decision Making\n",
    "\n",
    "A simple decision making situation $\\mathcal{D}$  \n",
    "* $Val(A) = \\{a^1,...,a^k\\}$ is a set of possible actions\n",
    "* $Val(X) = \\{x^1,...,x^n\\}$ is a set of states\n",
    "* $P(X \\mid A)$ is a distribution\n",
    "* $U(X, A)$ is a utility function the **Agent's preference**\n",
    "\n",
    "### 1.2 Expected Utility  \n",
    "$$EU[\\mathcal{D}[a]] = \\sum_x P(x \\mid z) U(x,a)$$\n",
    "\n",
    "Sum up over all possible states of the world that could be affected by the action, and then multiply by the utility of the stated-action.  \n",
    "\n",
    "This sum is the **overall happiness** of the decision  \n",
    "\n",
    "We want to choose the action $a$ that maximizes the expected utility.  This can be defined as:  \n",
    "\n",
    "$$a^* = argmax_a EU[\\mathcal{D}[a]]$$\n",
    "\n",
    "### 1.3 Influence Diagram\n",
    "We can make a model that is no longer a pure probabilistic model to represent this decision process.  \n",
    "\n",
    "<img src=\"./images/influence_diagram.png\">  \n",
    "\n",
    "The box represents the action variable where the Agent chooses the value of the variable. \n",
    "* These boxes are not random variables and therefore are not represented by a CPD.  \n",
    "\n",
    "The diamond $U$ is the utility  \n",
    "\n",
    "Computing the utility of the two choices in this graph  \n",
    "\n",
    "$$EU[\\mathcal{D}[a=f]] = \\sum_{m,a} P(m \\mid z=empty) U(a,m)$$  \n",
    "\n",
    "remember here $a = f$  \n",
    "\n",
    "$EU[F^0] = 0$  \n",
    "$EU[F^1] = (0.5*-7) + (0.3*5) + (0.2*20) = 2$  \n",
    "\n",
    "The optimal action would be to found a company.  \n",
    "\n",
    "### 1.4 Utility function can be decomposed\n",
    "\n",
    "<img src=\"./images/complex_influence_diagram.png\">  \n",
    "\n",
    "Decomposed Utility Function  \n",
    "$V_G$ = happiness of getting a good grade  \n",
    "$V_S$ = happiness of getting a good job  \n",
    "$V_Q$ = quality of life during the course  \n",
    "\n",
    "Assuming Grade, Job, Study and Difficulty are binary, how many values would you have to elicit for a **joint** utility function over all variables?\n",
    "\n",
    "You would need to elicit $2^4 = 16$ values since there are 4 binary variables.\n",
    "\n",
    "decomposition makes each utility function a CPD to avoid the exponential growth of large JPDs\n",
    "\n",
    "### 1.5 Information Edges\n",
    "\n",
    "<img src=\"./images/information_edge.png\">\n",
    "\n",
    "The edges that connect Parents to the **action variable** are considered **information edges**.  \n",
    "\n",
    "With this, we can define a **decision rule**, $\\delta$, at the action node $A$.  \n",
    "* This decision rule is effectively a CPD, $P(A \\mid Pa(A))$\n",
    "\n",
    "\n",
    "### 1.6 Expected Utility with Information\n",
    "\n",
    "$$EU[\\mathcal{D}[\\delta_A]] = \\sum_{x,a} P_{\\delta_{A}}(x \\mid a) U(x,a)$$\n",
    "\n",
    "$P_{\\delta_{A}}(x \\mid a)$ is a Joint Probability Distribution over $\\overline{X} \\cup [A]$\n",
    "\n",
    "The Agent wants to choose the decision rule $\\delta_A$ such that they obtain the **Maximum Expected Utility (MEU)**.  \n",
    "\n",
    "$$a^* = argmax_{\\delta_A} EU[\\mathcal{D}[\\delta_A]]$$  \n",
    "\n",
    "$$MEU(\\mathcal{D}) = max_{\\delta_A}EU[\\mathcal{D}[\\delta_A]]$$  \n",
    "\n",
    "### 1.7 Finding the MEU Decision Rules\n",
    "\n",
    "<img src=\"./images/optimize_delta.png\">\n",
    "\n",
    "We can write the expected utility as a product of factors such that:  \n",
    "\n",
    "$EU[\\mathcal{D}[\\delta_A]] = \\sum_{x,a} P_{\\delta_{A}}(x \\mid a) U(x,a)$  \n",
    "\n",
    "is the following expression in the Market-survey graph\n",
    "\n",
    "$\\sum_{M,S,F} P(M)P(S \\mid M) \\delta_{F}(F \\mid S) U(F,M)$  \n",
    "\n",
    "$\\sum_{S,F} \\delta_{F}(F \\mid S) \\sum_{M} P(M)P(S \\mid M)  U(F,M)$  \n",
    "\n",
    "After marginalizing for M we obtain a new factor $\\mu (F,S)$  \n",
    "\n",
    "$\\sum_{S,F} \\delta_{F}(F \\mid S) \\mu(F,S)$  \n",
    "\n",
    "The reason for using $\\mu$ is to suggest that this JPD has a utility component derived from $U$  \n",
    "\n",
    "#### Identify the decision rules with maximum utility\n",
    "\n",
    "<img src=\"./images/optimal_decision_rule.png\">  \n",
    "\n",
    "To Idenif the decision rule $\\delta_A$ we look at the CPD for the Agent given the information, $P(F \\mid S)$.  Here we find that the best decision is $(f^0 \\mid s^0)$, $(f^1 \\mid s^1)$, and $(f^1 \\mid s^2)$.  \n",
    "This information is converted to a decision rule that meets the following criteria:  \n",
    "\n",
    "$\\delta^{*}_{A}(a \\mid z) = \\begin{cases} 1 \\quad a = argmax_A \\mu(A,z) \\\\ 0 \\quad otherwise \\end{cases}$  \n",
    "\n",
    "Specifically this means the following:  \n",
    "\n",
    "* $\\delta_{F}(f=0 \\mid S=0) = 1$\n",
    "* $\\delta_{F}(f=0 \\mid S=1) = 0$\n",
    "* $\\delta_{F}(f=0 \\mid S=2) = 0$\n",
    "* $\\delta_{F}(f=1 \\mid S=0) = 0$\n",
    "* $\\delta_{F}(f=1 \\mid S=1) = 1$\n",
    "* $\\delta_{F}(f=1 \\mid S=2) = 1$\n",
    "\n",
    "In this case the optimal expected utility is:  \n",
    "\n",
    "$$MEU(\\mathcal{D}) = max_{\\delta_A}EU[\\mathcal{D}[\\delta_A]] = \\sum_{S,F} \\delta_{F}(F \\mid S) \\mu(F,S)$$\n",
    "\n",
    "$$\\sum_{S,F} \\delta_{F}(F \\mid S) \\mu(F,S) = P(f=0,s=0) + P(f=0,s=1) + P(f=0,s=2) + P(f=1,s=0) + P(f=1,s=1) + P(f=1,s=2)$$   \n",
    "\n",
    "$P(f=0,s=0) = \\delta_{F}(F \\mid S) \\mu(F,S) = 1.0 * 0.0 = 0$  \n",
    "$P(f=0,s=1) = \\delta_{F}(F \\mid S) \\mu(F,S) = 0.0 * 0.0 = 0$  \n",
    "$P(f=0,s=2) = \\delta_{F}(F \\mid S) \\mu(F,S) = 0.0 * 0.0 = 0$  \n",
    "$P(f=1,s=0) = \\delta_{F}(F \\mid S) \\mu(F,S) = 0.0 * -1.25 = 0$  \n",
    "$P(f=1,s=1) = \\delta_{F}(F \\mid S) \\mu(F,S) = 1.0 * 1.25 = 1.25$  \n",
    "$P(f=1,s=2) = \\delta_{F}(F \\mid S) \\mu(F,S) = 1.0 * 2.25 = 2.25$  \n",
    "\n",
    "$$MEU(\\mathcal{D}) = max_{\\delta_A}EU[\\mathcal{D}[\\delta_A]] = 0 + 0 + 0 + 0 + 1.25 + 2.25$$  \n",
    "\n",
    "When the Agent has access to the survey, the MEU is 3.25, which is higher than the MEU when the Agent doesn't have the survey.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+-----+-----+\n",
      "| M   | M_0 | M_0   | M_1 | M_1  | M_2 | M_2 |\n",
      "+-----+-----+-------+-----+------+-----+-----+\n",
      "| F   | F_0 | F_1   | F_0 | F_1  | F_0 | F_1 |\n",
      "+-----+-----+-------+-----+------+-----+-----+\n",
      "| S_0 | 0.0 | -2.1  | 0.0 | 0.45 | 0.0 | 0.4 |\n",
      "+-----+-----+-------+-----+------+-----+-----+\n",
      "| S_1 | 0.0 | -1.05 | 0.0 | 0.6  | 0.0 | 1.6 |\n",
      "+-----+-----+-------+-----+------+-----+-----+\n",
      "| S_2 | 0.0 | -0.35 | 0.0 | 0.45 | 0.0 | 2.0 |\n",
      "+-----+-----+-------+-----+------+-----+-----+\n",
      "u(S=0,F=0) = 0.00\n",
      "u(S=1,F=0) = 0.00\n",
      "u(S=2,F=0) = 0.00 \n",
      "\n",
      "u(S=0,F=1) = -1.25\n",
      "u(S=1,F=1) = 1.15\n",
      "u(S=2,F=1) = 2.10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.factors.discrete import DiscreteFactor\n",
    "\n",
    "cpd_M = TabularCPD('M',\n",
    "                    3,\n",
    "                    [[0.5, 0.3, 0.2]])\n",
    "\n",
    "cpd_S = TabularCPD('S',\n",
    "                    3,\n",
    "                    [[ 0.6,   0.3,  0.1],\n",
    "                     [ 0.3,   0.4,  0.4],\n",
    "                     [ 0.1,   0.3,  0.5]],\n",
    "                    evidence = ['M'],\n",
    "                    evidence_card=[3])\n",
    "\n",
    "cpd_U = DiscreteFactor(['F','M'],\n",
    "                       [2,3],\n",
    "                       [[ 0.0, 0, 0],\n",
    "                        [ -7,  5, 20]])\n",
    "\n",
    "P = cpd_S * cpd_M * cpd_U \n",
    "print P\n",
    "\n",
    "print 'u(S=0,F=0) = %.2f' %(0.0)\n",
    "print 'u(S=1,F=0) = %.2f' %(0.0)\n",
    "print 'u(S=2,F=0) = %.2f \\n' %(0.0)\n",
    "print 'u(S=0,F=1) = %.2f' %(-2.1+0.45+0.4)\n",
    "print 'u(S=1,F=1) = %.2f' %(-1.05+0.6+1.6)\n",
    "print 'u(S=2,F=1) = %.2f' %(-0.35+0.45+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.8 General Form of MEU and its Maximization\n",
    "<img src=\"./images/general_maximize_delta.PNG\">  \n",
    "\n",
    "### 1.9 MEU Algorithm Summary\n",
    "\n",
    "To compute MEU and optimize decision at $A$\n",
    "* Treat $A$ as a random variable with arbitrary CPD\n",
    "* Introduce Utility factor with scope $Pa_U$\n",
    "* Eliminate all variables except $A$, $Z$ ($A$'s parents) to produce factor $\\mu (A, Z)$ \n",
    "* For each $Z$, select:\n",
    "\n",
    "$\\delta^{*}_{A}(a \\mid z) = \\begin{cases} 1 \\quad a = argmax_A \\mu(A,z) \\\\ 0 \\quad otherwise \\end{cases}$  \n",
    "\n",
    "### 1.10 Decision Making Under Uncertainty  \n",
    "* MEU principle provides rigorous foundation\n",
    "* PGMs provide structured representation for probabilities, actions, and utilities\n",
    "* PGM inference methods (VE) can be used for \n",
    "    - finding the optimal strategy\n",
    "    - determining the overall value of the decision situation\n",
    "* Efficient methods also exist for \n",
    "    - multiple utility components\n",
    "    - multiple decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2. Utility Functions\n",
    "### 2.1 Utilities and Preferences\n",
    "\n",
    "Utility functions are necessary for our ability to compare complex scenarios that involve uncertainty or risk.  \n",
    "\n",
    "the way to formalize the decision making process of an Agent in these situations is to ascribe a numerical utility to the different outcomes.  \n",
    "\n",
    "For example:  \n",
    "\n",
    "Scenario A: \\$4 Million with P(0.2)   -or-  \\$0 with P(0.8)  \n",
    "Scenario B: \\$3 Million with P(0.25)  -or-  \\$0 with P(0.75)  \n",
    "\n",
    "Utility Scenario A = U(\\$4M) x 0.20 + U(\\$0) x 0.80 = \\$800,000  \n",
    "Utility Scenario B = U(\\$3M) x 0.25 + U(\\$0) x 0.75 = \\$750,000  \n",
    "\n",
    "We can then use the principle of MEU to determine the preferred scenario between these two scenarios.   \n",
    "\n",
    "#### utility preference is not linear\n",
    "\n",
    "It might seem that the preferred scenario between two scenarios is a linear function.  However, this is not the case.  \n",
    "\n",
    "Take the following scenarios:  \n",
    "\n",
    "Scenario A: \\$4 Million with P(0.8)  -or-  \\$0 with P(0.2)  \n",
    "Scenario B: \\$3 Million with P(1.0)  -or-  \\$0 with P(0.0)  \n",
    "\n",
    "Even though scenario A has a higher utility (\\$3.2 M), most people would chose scenario B.  \n",
    "\n",
    "### 2.2 St. Petersburg Paradox  \n",
    "* Fair coin is tossed repeatedly until it comes up heads, say on the $n^{th}$ toss.  \n",
    "* player is paid $\\$2^n$  \n",
    "\n",
    "The expected payout can be computed as:  \n",
    "\n",
    "$$P(T)*2 + P(TT) * 4 + P(TTT) * 8 ... = \\frac{1}{2}*2 + \\frac{1}{4}*4 + \\frac{1}{8} * 8 + ... = \\infty$$  \n",
    "\n",
    "In principle, a person might be willing to pay any amount to play this game, because no matter what they pay the expected utility is greater than that sum.  \n",
    "\n",
    "However, the reality is that the expected pay-out for anyone playing this game is approx. \\$2.  \n",
    "\n",
    "### 2.3 The Utility Curve\n",
    " The x-axis represents the \\$ dollar amount.   \n",
    " The y-axis is the utility that an Agent prescribes to an event where that dollar value is earned.  \n",
    " \n",
    " <img src=\"./images/utility_curve.PNG\">  \n",
    " \n",
    " In this plot, the solid line represents the utility of getting the dollar value **with certainty**. \n",
    " \n",
    " When we look at a series of lotteries where $D = \\begin{cases} \\$0 \\qquad with \\; prob. \\; 1-p \\\\ \\$1000 \\quad with \\; prob. \\; p \\end{cases}$  \n",
    " \n",
    "Because of the linearity of the utility, these lotteries are going to sit on the dotted line depending on the value of $p$.  \n",
    "\n",
    "In this case, \\$400 is called the **certainty equivalent** to the lottery in which $p = 0.5$. The certainty equivalent is the minimum amount of money you would prefer to take with 100 % certainty over the lottery.\n",
    "\n",
    "The difference $\\$500 - \\$400 = \\$100 $ is the **insurance premium**\n",
    "\n",
    "### 2.2 Risk Profiles\n",
    "**Risk Averse** - A Concave utility curve represent risk aversion.  \n",
    "\n",
    "**Risk Neutral** - A Linear utility curve.\n",
    "\n",
    "**Risk Seeking** - A Convex utility curve represents risk seeking behavior.  \n",
    "\n",
    "### 2.3 Typical Utility Curve  \n",
    "On the Gains/Earning side of the the Utility Curve, the preference is for risk aversion, getting money with certainty and a concave utility function.\n",
    "\n",
    "On the Loss side of the curve, the preference is risk seeking, or taking a small probability of a large loss as opposed to taking a smaller loss with certainty; this curve is convex. \n",
    "\n",
    "Near $ \\$ 0 $, the behavior is often risk neutral.\n",
    "\n",
    "### 2.4 Multi-Attribute Utility\n",
    "\n",
    "* All attributes affecting the person's preferences must be integrated into a single utility function (difficult to do b/c we may need to put human life on same scale as monitary gain).  \n",
    "    * Money\n",
    "    * Time\n",
    "    * Pleasure\n",
    "* How to bring human life into a utility function.\n",
    "    * wrong strategy is to bring the notion of a death into the function\n",
    "    * rather we use **Micromort** = 1/1,000,000 chance of mortality (in the 1980's a micromort was worth \\$20)\n",
    "    * QALY - Quality Adjusted Life Year  \n",
    "    \n",
    "**Example: Prenatal Testing**  \n",
    "Down's syndrome (D) - liklihood of having Down's  \n",
    "Testing (T) - Pain of testing for prenatal disorders  \n",
    "Knowledge (K) - Comfort of knowing test results, (what's going to happen)  \n",
    "Loss of fetus (L) - Testing may cause the loss of the fetus  \n",
    "Future pregnancy (F) - Whether there will be a future pregnancy or not (would you care if the baby has Down's if this is your last pregnancy)  \n",
    "\n",
    "Because there is structure in many people's utility function, these variables can be decomposed into the following Multi-attribute Utility Function:  \n",
    "\n",
    "$U(T) + U(K) + U(D,L) + U(L,F)$  \n",
    "\n",
    "If the variables T, K, D, L, and F are all binary valued variables, and the utility function is decomposed into $U(T) + U(K) + U(D,L) + U(L,F)$, how many different utility values have to be elicited to characterize the utility function?   \n",
    "\n",
    "Answer = 12, because we have $2 + 2 + 2*2 + 2*2 = 12$ (as opposed to 32 if we used a joint utility function over all variables).  \n",
    "\n",
    "### 2.5 Summary  \n",
    "* Our utility function determine our preference about decisions that involve uncertainty\n",
    "* Utility generally depends on multiple factors\n",
    "    * Money, time, chances of death, ...\n",
    "* Relationship is usually non-linear  \n",
    "    * Shape of utility curve determines attitude to risk \n",
    "* Multi-attribute utilities can help decompose high-dimensional functions into tractable pieces\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 3. Value of Perfect Information\n",
    "### 3.1 Value of Information  \n",
    "* $VPI(A \\mid X)$ Value of Perfect Information - is the value of observing $X$ before choosing an action $A$.  \n",
    "* $\\mathcal{D} =$ original influence diagram\n",
    "* $\\mathcal{D}_{X \\rightarrow A} =$ influence diagram after introducing edge $X \\rightarrow A$\n",
    "\n",
    "We can now define the Value of Perfect Information as:  \n",
    "\n",
    "$$VPI(A \\mid X) := MEU(\\mathcal{D}_{X \\rightarrow A}) - MEU(\\mathcal{D}) $$\n",
    "\n",
    "<img src=\"./images/VPI_1.PNG\"> \n",
    "\n",
    "### 3.2 Theorem\n",
    "\n",
    "Properties of VPI:  \n",
    "\n",
    "* $VPI(A \\mid X) \\geq 0$  \n",
    "* $VPI(A \\mid X) = 0$ i.i.f. the optimal decision role for $\\mathcal{D}$ is still optimal for $\\mathcal{D}_{X \\rightarrow A}$  \n",
    "\n",
    "The first theorem is true because the old influence diagram $MEU(\\mathcal{D})$ is is optimizing the decision rule $\\delta$ that satisfies the MEU of the CPD $\\delta(A \\mid \\bar{Z})$, and the new influence diagram $MEU(\\mathcal{D}_{X \\rightarrow A})$ is optimizing $\\delta(A \\mid \\bar{Z}, X)$.  \n",
    "\n",
    "$\\delta(A \\mid \\bar{Z}, X)$ is a strictly larger class of CPDs compared to $\\delta(A \\mid \\bar{Z})$.  Therefore, any decision rule implemented in the previous diagram,$\\delta(A \\mid \\bar{Z})$, can also be implemented in the current diagram, $\\delta(A \\mid \\bar{Z}, X)$.  The value of the decision in the previous diagram would have the same value as that same decision in the new diagram.  \n",
    "\n",
    "Therefore, one cannot possibly lose when exploring a space with a larger decision CPD.  \n",
    "\n",
    "For the second property, if the optimal decision in the original diagram, $\\delta(A \\mid \\bar{Z})$, is still the optimal decision in the new diagram, $\\delta(A \\mid \\bar{Z}, X)$, we have gained nothing from the additional observation $X$.  \n",
    "\n",
    "This gives us a clear definition of when information is useful.  \n",
    "\n",
    "For example:  \n",
    "\n",
    "Say we discover a magical lantern and upon rubbing it, a genie tells us that $\\delta(A \\vert \\overline{Z},X) \\neq \\delta(A \\vert \\overline{Z})$. What does this allow us to conclude about the value of knowing X?\n",
    "\n",
    "The value of knowing $X > 0$, because $\\delta(A \\vert \\overline{Z})$ is not optimal for $D_{X \\rightarrow A}$.  \n",
    "\n",
    "Simply put... **Information is useful precisely when it changes my decision in at least one case.**  \n",
    "\n",
    "### 3.3 Value of Information Example\n",
    "Recent graduate is trying to decide whether to join company 1 or company 2.  The diagram is as follows:  \n",
    "\n",
    "<img src=\"./images/VPI_2.PNG\">   \n",
    "\n",
    "Company 1 is more likely to be in a better state than company 2.\n",
    "\n",
    "We are assuming that the Agent's utility is 1 if the company gets funded and 0 if it does not.  \n",
    "\n",
    "$D = \\begin{cases} 1 \\qquad if \\; company \\; funded \\\\ 0 \\qquad if \\; company \\; not \\; funded \\end{cases}$  \n",
    "\n",
    "Based on this diagram (without any information):  \n",
    "\n",
    "$EU(\\mathcal{D}[c_1]) = 0.1*0.1 + 0.2*0.4 + 0.7*0.9 = 0.72$  \n",
    "$EU(\\mathcal{D}[c_2]) = 0.4*0.1 + 0.5*0.4 + 0.1*0.9 = 0.33$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+\n",
      "| F1   | F1_0 | F1_0 | F1_1 | F1_1 |\n",
      "+------+------+------+------+------+\n",
      "| C    | C_0  | C_1  | C_0  | C_1  |\n",
      "+------+------+------+------+------+\n",
      "| S1_0 | 0.0  | 0.0  | 0.01 | 0.0  |\n",
      "+------+------+------+------+------+\n",
      "| S1_1 | 0.0  | 0.0  | 0.08 | 0.0  |\n",
      "+------+------+------+------+------+\n",
      "| S1_2 | 0.0  | 0.0  | 0.63 | 0.0  |\n",
      "+------+------+------+------+------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpd_S1 = TabularCPD('S1',\n",
    "                    3,\n",
    "                    [[0.1, 0.2, 0.7]])\n",
    "\n",
    "cpd_F1 = TabularCPD('F1',\n",
    "                    2,\n",
    "                    [[ 0.9, 0.6, 0.1],\n",
    "                     [ 0.1, 0.4, 0.9]],\n",
    "                    evidence = ['S1'],\n",
    "                    evidence_card=[3])\n",
    "\n",
    "\n",
    "cpd_V = DiscreteFactor(['C','F1'],\n",
    "                       [2,2],\n",
    "                       [[0,1],\n",
    "                        [0,0]])\n",
    "\n",
    "P = cpd_S1 * cpd_F1 * cpd_V\n",
    "print P\n",
    "0.01 + 0.08 + 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+\n",
      "| F2   | F2_0 | F2_0 | F2_1 | F2_1 |\n",
      "+------+------+------+------+------+\n",
      "| C    | C_0  | C_1  | C_0  | C_1  |\n",
      "+------+------+------+------+------+\n",
      "| S2_0 | 0.0  | 0.0  | 0.0  | 0.04 |\n",
      "+------+------+------+------+------+\n",
      "| S2_1 | 0.0  | 0.0  | 0.0  | 0.2  |\n",
      "+------+------+------+------+------+\n",
      "| S2_2 | 0.0  | 0.0  | 0.0  | 0.09 |\n",
      "+------+------+------+------+------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpd_S2 = TabularCPD('S2',\n",
    "                    3,\n",
    "                    [[0.4, 0.5, 0.1]])\n",
    "\n",
    "cpd_F2 = TabularCPD('F2',\n",
    "                    2,\n",
    "                    [[ 0.9, 0.6, 0.1],\n",
    "                     [ 0.1,  0.4, 0.9]],\n",
    "                    evidence = ['S2'],\n",
    "                    evidence_card=[3])\n",
    "\n",
    "cpd_V = DiscreteFactor(['C','F2'],\n",
    "                       [2,2],\n",
    "                       [[0,0],\n",
    "                        [0,1]])\n",
    "\n",
    "P = cpd_S2 * cpd_F2 * cpd_V\n",
    "print P\n",
    "0.04 + 0.2 + 0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we allow the Agent to know information about company 2?  \n",
    "\n",
    "Specifically, we are adding a new edge to the diagram : $\\mathcal{D}_{State_2 \\rightarrow Company}$  \n",
    "\n",
    "The new decision rule can be summarized as:   \n",
    "\n",
    "$\\delta^* (C \\mid S_2) = \\begin{cases} P(c^2) =1 \\qquad if \\; S_2=s^3 \\\\ P(c^1) =1 \\qquad otherwise \\end{cases}$  \n",
    "\n",
    "Specifically this means that if $S_2 = s^1$ and $S_2 = s^2$ the utility of choosing $C_2$ is $0.1$ and $0.4$, respectively.  None of these choices are better than the utility of choosing $C_1$, which is $0.73$.  Only when $S_2 = s^3$ do we have a choice with better utility, $0.9$.  \n",
    "\n",
    "You just saw that the optimal decision only changes our mind if the second company is doing very well $(s^3)$. Recall that this only happens with prior probability 0.1. Do you think VPI will be high or low in this case?  \n",
    "\n",
    "Low -- The basic idea is that the gain in utility in this case has to be weighted by the probability of the state of the world that brings about that gain (i.e. the second company is doing very well). Because the probability of $s^3$ is not very large we should not expect a very large gain in expected utility so the value of the information will likely be small.  \n",
    "\n",
    "In fact, $MEU(\\mathcal{D}_{S2 \\rightarrow C}) = 0.738$  \n",
    "\n",
    "Therefore, the Agent shouldn't be willing to pay very much money for this additional information.  \n",
    "\n",
    "In another scenario, the probability of company 1 being in a good state is reduced as follows:  \n",
    "\n",
    "<img src=\"./images/VPI_3.PNG\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the optimal expected utility for the graph $\\mathcal{D}$ can be computed as:\n",
    "\n",
    "$\\mathcal{D} = \\delta(C \\mid S2) * P(S1) * P(S2) * P(F1 \\mid S1) * P(F2 \\mid S2) * V(F1, C, F2)$  \n",
    "\n",
    "$EU[\\mathcal{D}[\\delta_A]] = \\sum_{F1,F2,S1,S2,C} \\delta(C \\mid S2) * P(S1) * P(S2) * P(F1 \\mid S1) * P(F2 \\mid S2) * V(F1, C, F2)$  \n",
    "\n",
    "$EU[\\mathcal{D}[\\delta_A]] = \\sum_{S2,C} \\delta(C \\mid S2) * \\sum_{F1,F2,S1} P(S1) * P(S2) * P(F1 \\mid S1) * P(F2 \\mid S2) * V(F1, C, F2)$  \n",
    "\n",
    "$EU[\\mathcal{D}[\\delta_A]] = \\sum_{S2,C} \\delta(C \\mid S2) * \\sum_{F1,F2,S1} P(S2) * V(C)$  \n",
    "\n",
    "$EU[\\mathcal{D}[\\delta_A]] = \\sum_{S2,C} \\delta(C \\mid S2) * \\mu(S2,C)$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We set the value of Delta at the random variable, cpd_C, to be 1 when the decision is part of the\n",
      "Optimal Expected Utility (MEU) and 0 when the decision is not part of the MEU.\n",
      "\n",
      "This CPD, cpd_C, represents the Agents decision as to which company they will chose based \n",
      "on the MEU of chosing a company while knowing the state of company 2.\n",
      "\n",
      "Look carefully at cpd_C and factor_V.  cpd_C is a CPD that defines P(C|Pa(C)), and\n",
      "factor_V is a factor Phi(C,F1,F2).\n",
      "\n",
      "factor_V does not define a new random variable!\n",
      "The following matrix is the JPD before marginallizing for F1, F2, S1, S2, and C.\n",
      "The sum of this large JPD is the MEU of our decision rules Delta.\n",
      "MEU(D[delta]) = 0.7380\n"
     ]
    }
   ],
   "source": [
    "cpd_S1 = TabularCPD('S1',\n",
    "                    3,\n",
    "                    [[0.1, 0.2, 0.7]])\n",
    "\n",
    "cpd_F1 = TabularCPD('F1',\n",
    "                    2,\n",
    "                    [[ 0.9, 0.6, 0.1],\n",
    "                     [ 0.1, 0.4, 0.9]],\n",
    "                    evidence = ['S1'],\n",
    "                    evidence_card=[3])\n",
    "\n",
    "cpd_S2 = TabularCPD('S2',\n",
    "                    3,\n",
    "                    [[0.4, 0.5, 0.1]])\n",
    "\n",
    "cpd_F2 = TabularCPD('F2',\n",
    "                    2,\n",
    "                    [[ 0.9, 0.6, 0.1],\n",
    "                     [ 0.1, 0.4, 0.9]],\n",
    "                    evidence = ['S2'],\n",
    "                    evidence_card=[3])\n",
    "\n",
    "cpd_C = TabularCPD('C',\n",
    "                    2,\n",
    "                    [[1, 1, 0],\n",
    "                     [0, 0, 1]],\n",
    "                    evidence = ['S2'],\n",
    "                    evidence_card = [3])\n",
    "\n",
    "factor_V = DiscreteFactor(['C','F1', 'F2'],\n",
    "                       [2,2,2],\n",
    "                       [[0, 0, 1, 1],\n",
    "                        [0, 1, 0, 1]])\n",
    "\n",
    "P = cpd_C * cpd_S2  * factor_V * cpd_F2 * cpd_F1 * cpd_S1  \n",
    "# P.marginalize(['F1','F2','S1','V'])\n",
    "# print P # this is way to big to fit, looks better on a raw text editor such as `notepad`\n",
    "\n",
    "print 'We set the value of Delta at the random variable, cpd_C, to be 1 when the decision is part of the\\nOptimal Expected Utility (MEU) and 0 when the decision is not part of the MEU.\\n\\n\\\n",
    "This CPD, cpd_C, represents the Agents decision as to which company they will chose based \\n\\\n",
    "on the MEU of chosing a company while knowing the state of company 2.'\n",
    "\n",
    "print '\\nLook carefully at cpd_C and factor_V.  cpd_C is a CPD that defines P(C|Pa(C)), and\\n\\\n",
    "factor_V is a factor Phi(C,F1,F2).\\n\\n\\\n",
    "factor_V does not define a new random variable!'\n",
    "\n",
    "print 'The following matrix is the JPD before marginallizing for F1, F2, S1, S2, and C.'\n",
    "\n",
    "P.get_values() # this prints well here, but is uninterpretable.\n",
    "\n",
    "print 'The sum of this large JPD is the MEU of our decision rules Delta.'\n",
    "print 'MEU(D[delta]) = %.4f' %(np.sum(P.get_values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2:\n",
    "In another scenario we see what happens when the state of the first company is not as certain.  \n",
    "\n",
    "<img src=\"./images/VPI_4.PNG\">  \n",
    "\n",
    "The value of the information is higher in this scenario.  \n",
    "\n",
    "\n",
    "#### Example 3:\n",
    "In the last scenario we see what happens when the probability of funding is much higher across all states of a company.  \n",
    "\n",
    "<img src=\"./images/VPI_5.PNG\">  \n",
    "\n",
    "The value of the information is higher in this scenario.   \n",
    "\n",
    "### 3.4 Summary  \n",
    "\n",
    "Influence diagrams provide clear and coherent semantics for the value of making an observation.\n",
    "* difference between values of two ID's  (independent decisions).  \n",
    "\n",
    "Information is valuable i.i.f. it induces a change in action in at least one context.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
