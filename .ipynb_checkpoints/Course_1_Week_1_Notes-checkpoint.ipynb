{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1 Week 1 \n",
    "\n",
    "## Lecture 1. (Ch 2. in Probabilistic Graphical Models - Principles and Techniques)\n",
    "\n",
    "### 1.1 Basic Concepts in Probability\n",
    "\n",
    "#### some basic definitions\n",
    "outcome space = $\\Omega$  \n",
    "set of measurable events = $S$  \n",
    "event = $\\alpha$  \n",
    "empty set = $\\emptyset$  \n",
    "\n",
    "\n",
    "### 1.2 Random Variable\n",
    "A **Random Variable** is a function that associates a value with each outcome in omega.  \n",
    "\n",
    "Formally:  \n",
    "$\\{\\omega \\in \\Omega : f_{var}(\\omega) = A\\}$    \n",
    "  \n",
    "$Val(X)$ is the set of all values that a random variable $X$ can take\n",
    "  \n",
    "$\\sum_{i=1}^k P(X = x^i) = 1$  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2.  (Ch 2. in Probabilistic Graphical Models - Principles and Techniques)\n",
    "  \n",
    "### 2.1 Probability Distributions   \n",
    "$P(\\alpha) >= 0$ for all alpha in $S$  \n",
    "\n",
    "$P(\\Omega) = 1$  \n",
    "\n",
    "If $\\alpha$ and $\\beta$ are elements of S and $\\alpha$, and $\\beta$ do not equal $\\emptyset$, then $P(\\alpha \\cup \\beta) = P(\\alpha) + P(\\beta) - P(\\alpha \\cap \\beta)$  \n",
    "  \n",
    "### 2.2 Joint Distribution  \n",
    "A **Joint Distribution** is the distribution that assigns probabilities to events that are specified in terms of more than one random variable.  \n",
    "  \n",
    "Denoted as $P(X_1, ..., X_n)$  \n",
    "  \n",
    "The joint distribution must be consistent with the **marginal distribution**, such that $P(x) = \\sum_y P(x,y)$\n",
    "  \n",
    "How many variables are in a joint distribution?  \n",
    "\n",
    "In a joint distribution $P(X_1,X_2,â€¦,X_r)$, if each $X_i$ has $k_i$ values, the joint probability distribution has $k_i*k_{i+1}*...*K_r$ possible values to which a probability is assigned.  \n",
    "  \n",
    "Of these variables/parameters (e.g., 12 ) one less than the total (e.g., 11) are the number of independent parameters (e.g., 12 - 1 = 11 are independent parameters).  \n",
    "  \n",
    "### 2.3 Conditioning:  \n",
    "#### Reduction   \n",
    "\n",
    "Conditioning is called \"a reduction\" for the following reason:  \n",
    "\n",
    "If we have a probability distribution, $P(X_1,X_2,X_3)$, where all random variables are binary, and we say that $X_3 = 1$, such that we now have $P(X_1,X_2,X_3=1)$.  \n",
    "  \n",
    "This new probability distribution **no longer sums to 1.0 so conditioning is called a reduction**  \n",
    "  \n",
    "#### Renormalization  \n",
    "Renormalization of the reduced probability distribution changes the distribution to a valid probability distribution.  To do this, divide each entry of $P(X_1,X_2,X_3=1)$ by the sum of all the entries of $\\sum_{X_1,X_2}P(X_1,X_2,X_3=1)$.  \n",
    "  \n",
    "This process corresponds to the same distribution:    \n",
    "$$P(X_1,X_2 \\mid X_3 = 1)$$  \n",
    "  \n",
    "  \n",
    "### 2.4 Marginal Distribution   \n",
    "The **Marginal Distribution** is a distribution over events that can be described by the random variable X.  \n",
    "  \n",
    "Denoted as $P(X)$.    \n",
    "  \n",
    "If we have the joint distribution $P(I, G)$ and we marginalize I, then we obtain the marginal probability $P(G)$.  \n",
    "  \n",
    "### 2.5 Marginalization  \n",
    "**Marginalization** is the process of generating a marginal distribution from a joint distribution.  For instance, take the joint distribution $P(I, G)$  \n",
    "   \n",
    "The marginalization of $I$ for the joint distribution $P(G) = \\sum_I P(I,G)$.  \n",
    "  \n",
    "To do this we would sum the entries of D=0 and D=1 in the following table:  \n",
    "\n",
    "| I | G | prob|\n",
    "|---|---|-----|\n",
    "|i0 |g0 |0.282|  \n",
    "|i0 |g1 |0.02 |  \n",
    "|i1 |g0 |0.564|  \n",
    "|i1 |g1 |0.134|  \n",
    "  \n",
    "to obtain   \n",
    "  \n",
    "| G |prob                 |\n",
    "|---|---------------------|\n",
    "|g0 |0.282 + 0.564 = 0.846|  \n",
    "|g1 |0.020 + 0.134 = 0.154| \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3. (Ch 2. in Probabilistic Graphical Models - Principles and Techniques)\n",
    "### 3.1 Factors\n",
    "A **factor** is a function or a table usually denoted as $\\phi$ or $\\Phi$.  \n",
    "  \n",
    "It takes a random variable and gives us a value for every assignment of the random variable  \n",
    "  \n",
    "$$\\phi:Val(X_1, ..., X_k) \\rightarrow \\Re$$\n",
    "\n",
    "**Scope** is the set of arguments that the factor takes, ${X_1, X_2, ..., X_k}$\n",
    "\n",
    "#### Examples of factors\n",
    "* A joint distribution $P(I,D,G)$  \n",
    "  \n",
    "* An unnormalized measure $P(I,D,G=1)$ whose values do not sum to 1 (note: the scope is {I, D} because G is always 1)  \n",
    "  \n",
    "* A conditional Probability Distribution $P(G \\mid I,D)$  \n",
    "  \n",
    "* A General factor such as:  \n",
    "  \n",
    "| A | B | phi |\n",
    "|---|---|-----|\n",
    "|a0 |b0 |30   |\n",
    "|a0 |b1 |5    |\n",
    "|a1 |b0 |1    |\n",
    "|a1 |b1 |10   |\n",
    "  \n",
    "whose $Scope = {A,B}$    \n",
    "  \n",
    "### 3.2 Operations on Factors  \n",
    "#### Factor Product  \n",
    "The **Factor Product** is the product of two factors such as:  \n",
    "$$\\phi_1(A,B)*\\phi_2(B,C) = \\phi_3(A,B,C)$$  \n",
    "  \n",
    "This results in independent probabilities for the joint distribution $P(A,B,C)$  \n",
    "  \n",
    "If we want to obtain the factor product for a specific assignment to the variables in the scope of the factors, such as  $P(A=1, B=1, C=1)$, we would perform the following multiplication:  \n",
    "  \n",
    "$$P(A^1,B^1)*P(B^1,C^1) = P(A^1, B^1, C^1)$$  \n",
    "  \n",
    "#### Factor Marginalization\n",
    "Marginalization on factors works like marginalization on probability distributions (*see section 2.5 Marginalization*).  \n",
    "\n",
    "If we have a factor whose scope is ${A,B,C}$ and we want to remove $B$ from its scope, called marginalization of $B$, we sum the factor across all values of $B$.   \n",
    "  \n",
    "  $$\\phi(A,C) = \\sum_B \\phi(A,B,C)$$ , also\n",
    "  $$P(A,C) = \\sum_B P(A,B,C)$$  \n",
    "  \n",
    "#### Factor Reduction\n",
    "Reduction on factors works like reduction on probability distributions (*see section 2.3 Conditioning*).  \n",
    "\n",
    "When we want to obtain the values of the factor for a specific value of one of the variables in the factor's scope. for example:\n",
    "\n",
    "  $$\\phi(A,B,C=1) = \\phi(A,B,C^1)$$ , also\n",
    "  $$P(A,B,C=1) = P(A,B,C^1)$$  \n",
    "  \n",
    "the scope of the new factor is now ${A,B}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4. Semantics and Factorization  (Ch 3. in Probabilistic Graphical Models - Principles and Techniques)  \n",
    "\n",
    "### 4.1 Bayesian Network (Student Example)\n",
    "**Bayesian networks (BN)** are directed graphical models with nodes and edges.  Ultimately, we will learn how these models allow us to represent a joint probability distribution, such as $P(G,D,I,S,L)$, with fewer parameters than this joint distribution would require.  The way we can reduce the number of parameters is by leveraging independencies between variables.  These independencies are modeled by the structure of the Bayesian network.  We will come back to the concept of independencies in Lecture 7.  For now, we will focus on the structure of a Bayesian Network, such as the student example shown below:  \n",
    "\n",
    "<img src=\"./images/student_network.png\">\n",
    "  \n",
    "**node** - a variable/factor in the model  \n",
    "**edge** - a connection between nodes in the model, they can be directional or non-directional  \n",
    "  \n",
    "### 4.2 Fully parameterized Bayesian Network\n",
    "A **Fully Parameterized** network has all parameters filled in for the nodes of the mode.\n",
    "\n",
    "<img src=\"./images/student_network_with_CPDs.png\">\n",
    "  \n",
    "* The nodes D, and I are represented by **unconditional probability distributions** (also joint probability distributions)\n",
    "  \n",
    "* The nodes G, S, and L are represented by CPDs   \n",
    "  \n",
    "* Each row in the CPDs of the network sum to 1.0  \n",
    "  \n",
    "### 4.3 Chain Rule for Bayesian Networks\n",
    "The **Chain Rule** is a **Factor Product** of all the CPDs of the Bayesian Network.  \n",
    "  \n",
    "We can always factor a Joint Probability Distribution using the general chain rule:\n",
    "  \n",
    "$$P(X, Y) = P(X)*P(Y \\mid X)$$\n",
    "\n",
    "This can be extended to multiple variables as:\n",
    "\n",
    "$$P(X_1,...,X_k) = P(X_1)*P(X_2 \\mid X_1)*...*P(X_k \\mid X_1,...,X_{k-1})$$\n",
    "\n",
    "In a BN, the chain rule is similar but different. The probability distributions in the BN are written as they appear in the BN.  In other words we would write the CPD for a node in the BN as the probability $P$ of the random variable after which the node is named, conditioned on the parents of the node. For Grade we have $P(G \\mid Pa(G)) = P(G \\mid I,D)$. $Pa$ is the notation for \"parents\". With these CPDs written down for all nodes, we then simply multiply all these CPDs together to achieve the joint probability distribution across the BN. In the chain rule results in the following equation for the Student example:  \n",
    "  \n",
    "$$P(G,D,I,S,L)=P(D)*P(I)*P(G \\mid I,D)*P(S \\mid I)*P(L \\mid G)$$\n",
    "  \n",
    "This factor product gives us a larger factor (i.e., joint probability distribution) whose scope is ${G,D,I,S,L}$  \n",
    "  \n",
    "This large factor product is the Unconditional Probability Distribution $P(G,D,I,S,L)$   \n",
    "\n",
    "### 4.4 Using the Chain Rule  \n",
    "compute $P(d^0, i^1, g^3, s^1, l^1)$  \n",
    "\n",
    "Based on the Chain Rule we have:   \n",
    "  \n",
    "$P(G,D,I,S,L)=P(D)*P(I)*P(G \\mid I,D)*P(S \\mid I)*P(L \\mid G)$  \n",
    "\n",
    "$P(d^0) = 0.6$   \n",
    "$P(i^1) = 0.3$  \n",
    "$P(g^3) = P(g^3 \\mid i^1, d^0) = 0.02$  \n",
    "$P(s^1) = P(s^1 \\mid i^1) = 0.8$  \n",
    "$P(l^1) = P(l^1 \\mid g^3) = 0.01$  \n",
    "\n",
    "$P(d^0, i^1, g^3, s^1, l^1)=0.6*0.3*0.02*0.8*0.01$   \n",
    "  \n",
    "### 4.5 Definition of Bayesian Network  \n",
    "\n",
    "* A **directed acyclic graph (DAG)**, $G$, whose nodes represent the random variables $X_1, ..., X_n$  \n",
    "* for each node $X_i$ a CPD $P(X_i \\mid Par_G(X_i))$ (Note: Par stands for Parents)  \n",
    "* the BN represents a **Joint Distribution** via the Chain Rule for Bayesian Networks:  \n",
    "$$P(X_i, ..., X_n) = \\prod_i P(X_i \\mid Par_G(X_i)) $$\n",
    "\n",
    "### 4.6 A BN is a Legal Distribution  \n",
    "#### $P \\geq\\ 0$\n",
    "* The Joint Distribution of the BN defined by the chain rule is a CPD   \n",
    "* This CPD is defined by the product of other CPDs    \n",
    "* The other CPDs are non-negative, therefore P is positive  \n",
    "\n",
    "#### $\\sum P = 1$  \n",
    "We can show that this sum equals 1  \n",
    "  \n",
    "$\\sum_{D,I,G,S,L}P(G,D,I,S,L)=\\sum_{D,I,G,S,L}P(D)*P(I)*P(G \\mid I,D)*P(S \\mid I)*P(L \\mid G)$  \n",
    "  \n",
    "$=\\sum_{D,I,G,S}P(D)*P(I)*P(G \\mid I,D)*P(S \\mid I)*\\sum_L P(L \\mid G)$  \n",
    "  \n",
    "$=\\sum_{D,I,G}P(D)*P(I)*P(G \\mid I,D)*\\sum_S P(S \\mid I)*1.0$  \n",
    "  \n",
    "$=\\sum_{D,I}P(D)*P(I)*\\sum_G P(G \\mid I,D)*1.0*1.0$  \n",
    "...  \n",
    "$=1*1*1*1*1$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5. Reasoning Patterns (Ch 3. in Probabilistic Graphical Models - Principles and Techniques)\n",
    "\n",
    "### 5.1 Causal Reasoning  \n",
    "Reasoning goes in a causal direction $A \\rightarrow B \\rightarrow C$, also top-down.    \n",
    "  \n",
    "Causal Reasoning examples from the Student Model:  \n",
    "\n",
    "* $P(l^1 \\mid i^0) = 0.39$\n",
    "* $P(l^1 \\mid i^0, d^0) = 0.51$\n",
    "\n",
    "How to compute these values? \n",
    "\n",
    "Don't sweat this just yet, we need to learn a technique called variable elimination to do this without computing the entire joint probability distribution.  We will learn this technique later in the second course.  \n",
    "\n",
    "For now, we can use the Python library `pgmpy`.  The computation of these values is provided in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| L   |   phi(L) |\n",
      "|-----+----------|\n",
      "| L_0 |   0.6114 |\n",
      "| L_1 |   0.3886 |\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "| L   |   phi(L) |\n",
      "|-----+----------|\n",
      "| L_0 |   0.4870 |\n",
      "| L_1 |   0.5130 |\n",
      "+-----+----------+\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "model = BayesianModel()\n",
    "model.add_nodes_from(['D','I', 'G', 'S', 'L'])\n",
    "model.add_edges_from([('D','G'), ('I','G'), ('I','S'), ('G','L')])\n",
    "\n",
    "cpd_D = TabularCPD('D',\n",
    "                    2,\n",
    "                    [[0.6,0.4]])\n",
    "\n",
    "cpd_I = TabularCPD('I',\n",
    "                    2,\n",
    "                    [[0.7,0.3]])\n",
    "\n",
    "cpd_S = TabularCPD('S',\n",
    "                    2,\n",
    "                    [[0.95, 0.2],\n",
    "                     [0.05, 0.8]],\n",
    "                    evidence = ['I'],\n",
    "                    evidence_card=[2])\n",
    "\n",
    "cpd_G = TabularCPD('G',\n",
    "                    3,\n",
    "                    [[ 0.3,   0.05,  0.9,   0.5 ],\n",
    "                     [ 0.4,   0.25,  0.08,  0.3 ],\n",
    "                     [ 0.3,   0.7,   0.02,  0.2 ]],\n",
    "                    evidence = ['I','D'],\n",
    "                    evidence_card=[2,2])\n",
    "\n",
    "cpd_L = TabularCPD('L',\n",
    "                    2,\n",
    "                    [[ 0.1,   0.4,   0.99],\n",
    "                     [ 0.9,   0.6,   0.01]],\n",
    "                    evidence = ['G'],\n",
    "                    evidence_card=[3])\n",
    "\n",
    "model.add_cpds(cpd_D,\n",
    "               cpd_I,\n",
    "               cpd_S,\n",
    "               cpd_G,\n",
    "               cpd_L)\n",
    "\n",
    "model.check_model()\n",
    "\n",
    "infer = VariableElimination(model)\n",
    "\n",
    "print(infer.query(['L'], evidence={'I': 0}) ['L'])\n",
    "print(infer.query(['L'], evidence={'I': 0, 'D': 0}) ['L'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+-----+\n",
      "| I   | I_0 | I_0  | I_1  | I_1 |\n",
      "+-----+-----+------+------+-----+\n",
      "| D   | D_0 | D_1  | D_0  | D_1 |\n",
      "+-----+-----+------+------+-----+\n",
      "| G_0 | 0.3 | 0.05 | 0.9  | 0.5 |\n",
      "+-----+-----+------+------+-----+\n",
      "| G_1 | 0.4 | 0.25 | 0.08 | 0.3 |\n",
      "+-----+-----+------+------+-----+\n",
      "| G_2 | 0.3 | 0.7  | 0.02 | 0.2 |\n",
      "+-----+-----+------+------+-----+\n"
     ]
    }
   ],
   "source": [
    "print cpd_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evidential Reasoning\n",
    "Reasoning goes in an evidential direction $A \\leftarrow B \\leftarrow C$, also bottom-up.  \n",
    "  \n",
    "Evidential Reasoning examples from the Student Model:   \n",
    "The student gets a C in the class  \n",
    "* $P(d^1 \\mid g^3) = 0.63$\n",
    "* $P(i^1 \\mid g^3) = 0.03$\n",
    "\n",
    "\n",
    "Note: We do know how to compute these conditional probabilities, because $D$ and $G$ are directly connected.  We use the definition of conditional probability, $P(D  \\mid G) = \\frac{P(D,G)}{P(G)}$, and the chain rule for a BN.\n",
    "\n",
    "1. First, write down the expression to compute this probability $P(d^1 \\mid g^3) = \\frac{P(d^1,g^3)}{P(g^3)}$\n",
    "2. Generate the joint probability distribution $P(D,G)$ by the following $P(D,G) = P(G \\mid I,D)*P(I)*P(D)$\n",
    "3. Now we can use this new joint probability table to compute $P(d^1,g^3)$ and $P(g^3)$\n",
    "5. Perform the computation in in step 1 with these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+--------+-------+\n",
      "| I   | I_0   | I_0   | I_1    | I_1   |\n",
      "+-----+-------+-------+--------+-------+\n",
      "| D   | D_0   | D_1   | D_0    | D_1   |\n",
      "+-----+-------+-------+--------+-------+\n",
      "| G_0 | 0.126 | 0.014 | 0.162  | 0.06  |\n",
      "+-----+-------+-------+--------+-------+\n",
      "| G_1 | 0.168 | 0.07  | 0.0144 | 0.036 |\n",
      "+-----+-------+-------+--------+-------+\n",
      "| G_2 | 0.126 | 0.196 | 0.0036 | 0.024 |\n",
      "+-----+-------+-------+--------+-------+\n",
      "\n",
      "P(D=1 | G=2) / P(G=2) = (0.196 + 0.024) / (0.126 + 0.196 + 0.0036 + 0.024)\n",
      "\n",
      "0.629290617849\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.factors.discrete import TabularCPD\n",
    "\n",
    "cpd_D = TabularCPD('D',\n",
    "                    2,\n",
    "                    [[0.6,0.4]])\n",
    "\n",
    "cpd_I = TabularCPD('I',\n",
    "                    2,\n",
    "                    [[0.7,0.3]])\n",
    "\n",
    "cpd_G = TabularCPD('G',\n",
    "                    3,\n",
    "                    [[ 0.3,   0.05,  0.9,   0.5 ],\n",
    "                     [ 0.4,   0.25,  0.08,  0.3 ],\n",
    "                     [ 0.3,   0.7,   0.02,  0.2 ]],\n",
    "                    evidence = ['I','D'],\n",
    "                    evidence_card=[2,2])\n",
    "\n",
    "P = cpd_G*cpd_D*cpd_I\n",
    "print P\n",
    "\n",
    "print '\\nP(D=1 | G=2) / P(G=2) = (0.196 + 0.024) / (0.126 + 0.196 + 0.0036 + 0.024)\\n'\n",
    "print (0.196 + 0.024) / (0.126 + 0.196 + 0.0036 + 0.024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Intercausal Reasoning\n",
    "This type of reasoning is present when we have what is called a \"V-structure\" in the graph. In Intercausal Reasoning, we look at the flow of information between two causes across this v-structure, $A \\rightarrow B \\leftarrow C$.  \n",
    "  \n",
    "A simple Intercausal Reasoning example from the Student Model:  \n",
    "  \n",
    "What is the probability that a student is intelligent given the student gets a C grade in the class, and the class is difficult\n",
    "* $P(i^1 \\mid g^3, d^1) = 0.11$  \n",
    "\n",
    "We will come to understand the importance of these V-structures in the next lecture, but for now it is important that $I$ is not independent of $D$ given $G$ when the value of $G$ is known. Let's wait to understand the importance of that statement till the next lecture, for now we should look at an example of how this works.\n",
    "\n",
    "We can explain how this works using the deterministic OR model:  \n",
    "  \n",
    "<img src=\"./images/deterministic_OR_model.PNG\">  \n",
    "  \n",
    "Without knowing the value of $Y$, we can see that $P(X_1^1) = P(X_1^1 \\mid X_2^1) = 1/2$.  \n",
    "\n",
    "Now, let's Condition on $P(X_1, X_2, Y=1)$  \n",
    "  \n",
    "We can show that $X_1$ and $X_2$ are no longer independent of each other, because $P(X_1^1 \\mid Y^1) = 2/3$ yet $P(X_1^1 \\mid X_2^1,Y^1) = 1/2$.  This fails the definition of independence given in Lecture 7.1.\n",
    "\n",
    "In that last statement, we saw that by adding another condition, that both $X_2=1$ and  $Y=1$, the probability that $P(X_1^1 \\mid X_2^1, Y^1)$ goes back to being $1/2$  \n",
    "  \n",
    "The reason is that if we know that $X_2=1$, $Y$ must equal $1$. The table shows this clearly. We say that we have *completely explained away* the evidence for why the variable $Y=1$.  Since we now have a *complete* explanation as to why $Y=1$, the $P(X_1=1)$ goes back to being independent, e.g., 1/2.\n",
    "  \n",
    "This particular type of Intercausal Reasoning is called **Explaining Away**  \n",
    "  \n",
    "### 5.4 long stretches of reasoning\n",
    "In this example we have a long stretch of reasoning: $D \\rightarrow G \\leftarrow I \\rightarrow S$\n",
    "\n",
    "What happens to the probability that $I=1$ if we know that the student aced the SAT, but got a C grade in the class?  \n",
    "  \n",
    "$P(i^1 \\mid s^1, g^3)$  \n",
    "  \n",
    "We can see that $P(i=1)$ increases, but what about $P(d=1)$?  \n",
    "  \n",
    "$P(d=1)$ actually goes up as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 6. Flow of Probabilistic Influence\n",
    "### 6.1 When can X influence Y\n",
    "$X \\rightarrow Y$  **Yes**  \n",
    "$X \\leftarrow Y$  **Yes**  \n",
    "$X \\rightarrow W \\rightarrow Y$  **Yes**  \n",
    "$X \\leftarrow W \\leftarrow Y$  **Yes**  \n",
    "$X \\leftarrow W \\rightarrow Y$  **Yes**  \n",
    "$X \\rightarrow W \\leftarrow Y$  **No**  Called a V-structure\n",
    "   \n",
    "### 6.2 Trails\n",
    "**Trail** - A sequence of nodes combined by edges in the graph; $X_1 - ... - X_k$  \n",
    "  \n",
    "### 6.3 When can X influence Y given evidence about Z\n",
    "\n",
    "$W \\notin Z$  \n",
    "  \n",
    "$X \\rightarrow Y$  **Yes**  \n",
    "$X \\leftarrow Y$  **Yes**  \n",
    "$X \\rightarrow W \\rightarrow Y$  **Yes**  \n",
    "$X \\leftarrow W \\leftarrow Y$  **Yes**  \n",
    "$X \\leftarrow W \\rightarrow Y$  **Yes**  \n",
    "$X \\rightarrow W \\leftarrow Y$  **No, if W and all of its descendants are not observed**\n",
    "  \n",
    "$W \\in Z$  \n",
    "  \n",
    "$X \\rightarrow Y$  **Yes**  \n",
    "$X \\leftarrow Y$  **Yes**  \n",
    "$X \\rightarrow W \\rightarrow Y$  **No**  \n",
    "$X \\leftarrow W \\leftarrow Y$  **No** (probabilistic influence is symmetrical)  \n",
    "$X \\leftarrow W \\rightarrow Y$  **No**  \n",
    "$X \\rightarrow W \\leftarrow Y$  **Yes, if W or one of its descendants is observed**  (this is the case of **intercausal reasoning**)  \n",
    "  \n",
    "### 6.4 Flow of reasoning\n",
    "When can S - I - G - D allow influence to flow?  \n",
    "  \n",
    "Specifically for the student case we have $S \\leftarrow I \\rightarrow G \\leftarrow D$\n",
    "* If $I$ is observed - **No**\n",
    "* $I$ not observed nor is anything else - **No** (V-structure causes this flow to stop)\n",
    "* $I$ not observed $G$ is observed - **yes** (with $G$ observed, V-structure is bridged)\n",
    "\n",
    "### 6.2 Complete Definition of Active Trails\n",
    "**Trail** - A sequence of nodes combined by edges in the graph; $X_1 - ... - X_k$  \n",
    "  \n",
    "**Active Trail**\n",
    "* A trail is active if no variables are observed ($X_i \\notin Z$), and it has no V-structures such as $X_{i-1} \\rightarrow X_i \\leftarrow X_{i+1}$  \n",
    "  \n",
    "* For any structure $X_{i-1} \\rightarrow X_i \\leftarrow X_{i+1}$ we have observed $X_i$ (i.e., the variable at the vertex of the V-structure) or one of its descendants, and no other $X_i$ is observed  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 7. Conditional Independence\n",
    "### 7.1 Independence\n",
    "For events $\\alpha, \\beta, P\\models \\alpha \\bot \\beta$ in other words, \"P satisfies alpha's independence of beta\"  \n",
    "  \n",
    "$\\models$ is the symbol for **satisfies**  \n",
    "  \n",
    "$\\bot$ is the symbol for **independence**  \n",
    "  \n",
    "$\\alpha, \\beta, P\\models \\alpha \\bot \\beta$ if:\n",
    "  \n",
    "* $P(\\alpha, \\beta) = P(\\alpha)*P(\\beta)$  \n",
    "* $P(\\alpha \\mid \\beta) = P(\\alpha)$  \n",
    "* $P(\\beta \\mid \\alpha ) = P(\\beta)$ (because probabilistic influence is symmetrical)  \n",
    "(note: $ P(\\alpha, \\beta) == P(\\alpha \\cap \\beta)$)  \n",
    "\n",
    "#### for random variables\n",
    "* $P(X, Y) = P(X)*P(Y)$  \n",
    "* $P(X \\mid Y) = P(X)$  \n",
    "* $P(Y \\mid X) = P(Y)$ (because probabilistic influence is symmetrical)  \n",
    "\n",
    "#### universal statement\n",
    "for every assignment x, and y to the variables X, and Y (noted as: $\\forall x,y$) we have that $P(x,y) = P(x)*P(y)$\n",
    "\n",
    "#### Example\n",
    "in the Student Model, the joint probability distribution P(I,D) is equal to the product of the two marginal distributions I and D:  $P(I,D) = P(I)*P(D)$  \n",
    "  \n",
    "### 7.2 Conditional Independence\n",
    "$P\\models X \\bot Y \\mid Z$  \n",
    "  \n",
    "* $P(X, Y \\mid Z) = P(X \\mid Z)*P(Y \\mid Z)$  \n",
    "* $P(X \\mid Y, Z) = P(X \\mid Z)$  \n",
    "* $P(Y \\mid X, Z) = P(Y \\mid Z)$\n",
    "* $P(X, Y, Z) \\propto \\phi_1(X, Z)*\\phi_2(Y, Z)$ ($\\propto$ means proportional to; i.e., we can forget the need to add normalizing constants; i.e., we can use the raw Factor rather than the Probability distribution)\n",
    "\n",
    "#### Example\n",
    "<img src=\"./images/coin_model.PNG\">\n",
    "\n",
    "$C_1$ = fair coin  \n",
    "$C_2$ = biased coin; where $P(C_2 = heads) > P(C_2 = tails)$  \n",
    "  \n",
    "If we don't know which coin was picked, but we know that the first toss was a heads, the probability that the second toss is heads is higher. \n",
    "  \n",
    "$P(X_2 = Heads \\mid X_1 = heads) > 0.5$  \n",
    "\n",
    "Besides reasoning, we know this is true because the trail from $X_1 \\leftarrow C \\rightarrow X_2$ is active \n",
    "\n",
    "\n",
    "**However**, if we know that the coin is the biased coin $C_2$, knowing that the first toss was a heads provides no change in the probability that the second toss was a heads.  \n",
    "$P(X_2 = heads \\mid C = C_2, X_1 = heads) = P(X_2 = heads \\mid C = C_2)$  \n",
    "  \n",
    "Besides reasoning, we know this is true because the trail from $X_1 \\leftarrow C \\rightarrow X_2$ is not active, because $C \\in$ our evidence.  \n",
    "  \n",
    "Formally:  \n",
    "$$P \\models (X_1 \\bot X_2 \\mid C)$$  \n",
    "  \n",
    "### 7.3 Conditioning can Lose Independences\n",
    "In the student example, I and D are independent in the distribution, but they are no longer independent when we know G.  \n",
    "  \n",
    "I - G - D has a V-structure:  \n",
    "  \n",
    "$I \\rightarrow G \\leftarrow D$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 8. Independencies in Bayesian Networks  \n",
    "### 8.1 d-separation  \n",
    "Definition: X and Y are d-separated in G given Z if there is **no active trail** in G between X and Y given Z. In other words, if all paths from a vertex of A to a vertex of B are blocked w.r.t. (with respect to) to C.\n",
    "  \n",
    "d-separation implies independence  \n",
    "  \n",
    "Rules that define blocking:  \n",
    "**blocked if the vertex B is being conditioned on**  \n",
    "* $A \\rightarrow B \\rightarrow C$  \n",
    "* $A \\leftarrow B \\leftarrow C$  \n",
    "* $A \\rightarrow B \\leftarrow C$   \n",
    "\n",
    "**Blocked if the vertex B or any of B's descendants are **not** being conditioned on**  \n",
    "* $A \\rightarrow B \\leftarrow C$   \n",
    "  \n",
    "<img src=\"./images/d_separation.PNG\"> \n",
    "https://www.youtube.com/watch?v=aA-gTNxy1rw\n",
    " \n",
    "Theorem: If P factorizes over G, and $X$ and $Y$ are d-separated in $Z$ (i.e. $d-sep_G(X,Y \\mid Z)$), then P satisfies $(X \\bot Y \\mid Z)$.  \n",
    "\n",
    "Notation: $d-sep_G(X,Y \\mid Z)$   \n",
    "  \n",
    "<img src=\"./images/flow_of_influence.PNG\">  \n",
    "  \n",
    "Let's prove that $D \\bot S$ in our student network\n",
    "\n",
    "By the chain rule we know that:  \n",
    "$P(G,D,I,S,L)=P(D)*P(I)*P(G \\mid I,D)*P(S \\mid I)*P(L \\mid G)$  \n",
    "The joint distribution for D and S is defined by:\n",
    "$P(D,S)=\\sum_{G,L,I}P(D)*P(I)*P(G \\mid I,D)*P(S \\mid I)*P(L \\mid G)$    \n",
    "$P(D,S)=\\sum_{I}P(D)*P(I)*P(S \\mid I)*\\sum_{G}P(G \\mid I,D)*\\sum_{L}P(L \\mid G)$    \n",
    "$P(D,S)=P(D)*\\sum_{I}P(I)*P(S \\mid I)*1.0*1.0$    \n",
    "$P(D,S)=P(D)*1.0*P(S)*1.0*1.0$    \n",
    "$P(D,S)=P(D)*P(S)$    \n",
    "This proves that $P \\models D \\bot S$\n",
    "\n",
    "#### Any Node is d-separated from its non-descendants given its parents\n",
    "Any trail into L that comes in through its parents (i.e., G) will be blocked\n",
    "\n",
    "If P factorizes over G, then in P, any variable is independent of its non-descendants given its parents\n",
    "\n",
    "Example: The variable L is always independent of the variables D, I, and S when G is observed. \n",
    "\n",
    "### 8.2 Independence Map (I-maps)  \n",
    "#### definition  \n",
    "  All of the independencies in G are implied by all of the independence statements that correspond to d-separation statements in the graph.  \n",
    "$$I(G) =\\{(X \\bot Y \\mid Z):d-sep_G(X,Y \\mid Z)\\}$$  \n",
    "  \n",
    "Definition: if $P$ satisfies $I(G)$, then $G$ is an I-map of $P$  \n",
    "  \n",
    "This definition does note require that $I(G)$ exactly represent the independencies in a graph G.  Therefore $I(G)= \\emptyset$ is an I-map for any graph.  \n",
    "\n",
    "#### Theorem  \n",
    "If P factorizes over G, then G is an I-map for P (i.e., we can read from G independenceis that hold in P regardless of the parameters)\n",
    "\n",
    "For example:  \n",
    "Using the chain rule for Joint Probability Distributions:  \n",
    "$P(D,I,G,S,L)=P(D)P(I \\mid D)P(G \\mid D,I)P(S \\mid D,I,G)P(L \\mid D,I,G,S)$  \n",
    "using independence rules reduces to:\n",
    "$P(G,D,I,S,L)=P(D)*P(I)*P(G \\mid I,D)*P(S \\mid I)*P(L \\mid G)$    \n",
    "\n",
    "### 8.3 Summary\n",
    "* Factorization: G allows P to be represented\n",
    "* I-map: Independencies encoded by G hold in P\n",
    "* If P factorizes over a graph G, we can read from the graph independencies that must hold in P (an I-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 9. Naive Bayes  \n",
    "  \n",
    "The only independence assumption made in the Naive Bayes model is: given the class variable, all features are independent of one another... $ P \\models X_i \\bot X_j \\mid C$  \n",
    "  \n",
    "This allows us to encode the chain rule for Naive Bayes Model in a simple way:\n",
    "$$P(C, X_1, ..., X_n)=P(C)\\prod_{i=1}^n P(X_i \\mid C)$$\n",
    "\n",
    "#### Ratio of the probability of two classes\n",
    "$$\\frac{P(C=c^1 \\mid x_1, ..., x_n)}{P(C=c^2 \\mid x_1, ..., x_n)}$$\n",
    "\n",
    "This can be broken down into two terms:\n",
    "* the ratio of the prior probabilities of the two classes\n",
    "$$\\frac{P(C=c^1)}{P(C=c^2}$$\n",
    "* a product of [odds ratios][1]; the probability of seeing a particular observation $x_i$ in the context of one class relative to the other class\n",
    "$$\\prod^{n}_{i=1}\\frac{P(x_i \\mid C=c^1)}{P(x_i \\mid C=c^2)}$$\n",
    "\n",
    "All together we have  \n",
    "\n",
    "$$\\frac{P(C=c^1 \\mid x_1, ..., x_n)}{P(C=c^2 \\mid x_1, ..., x_n)} = \\frac{P(C=c^1)}{P(C=c^2)}\\prod^{n}_{i=1}\\frac{P(x_i \\mid C=c^1)}{P(x_i \\mid C=c^2)}$$\n",
    "\n",
    "[1]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2938757/  \n",
    "\n",
    "### 9.1 Bernoulli Naive Bayes for Text  \n",
    "Each variable is a binary variable subject to a Bernoulli Distribution.  \n",
    "\n",
    "Naive Bayes, because we make very strong independence assumptions that the event of one word appearing is independent of the event of a different word appearing given that we know the class. \n",
    "\n",
    "Imagine trying to determine the type of document we have given a list of unique words in the document.\n",
    "\n",
    "for each word in the dictionary, we have a binary random variable:\n",
    "* 1 - if word in dictionary appears in the document\n",
    "* 0 - if word does not\n",
    "    \n",
    "If there are 5,000 words in our dictionary, we would have 5,000 of these random variables  \n",
    "\n",
    "<img src=\"./images/bernoulli_naive_bayes.PNG\"> \n",
    "\n",
    "### 9.2 Multinomial Naive Bayes for Text  \n",
    "the variables that represent the features are words in the document; $n$ is the length of the document.  \n",
    "\n",
    "The values of these random variables are an actual word, i.e., # 348 in the dictionary of 5,000.  \n",
    "\n",
    "Assume the probability distribution at each random variable is the same: A Multinomial Distribution where each entry in the rows sum to 1.0.  \n",
    "\n",
    "Despite there being many 2-word phrases, wherein the assumption of independence in a Naive Bayes may seem too strong, this model works quite well.  \n",
    "\n",
    "### 9.3 Summary  \n",
    "Simple approach for classification.  \n",
    "* computationally efficient  \n",
    "* easy to construct  \n",
    "Surprisingly Effective in domains with may weakly relevant features.  \n",
    "Strong independence assumptions reduce performance when many features are strongly correlated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
