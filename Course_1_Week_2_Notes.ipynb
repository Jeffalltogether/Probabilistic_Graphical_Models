{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# # Course 1 Week 2\n",
    "## Lecture 1. Overview of Template Models\n",
    "### 1.0 Sharing Between Models & Sharing Within Models\n",
    "\n",
    "#### Image Segmentation  \n",
    "The same model can be shared for every pixel within an image  \n",
    "\n",
    "The same model can be shared between images of the same database  \n",
    "\n",
    "<img src=\"./images/image_segmentation.png\">  \n",
    "\n",
    "### 1.1 Template Variable\n",
    "Template variable $X(U_1, ..., U_k)$ is instantiated (duplicated) multiple times.\n",
    "\n",
    "* these variables are indexed by various things, for instance: timepoints, people, pixels, courses, students, etc.  \n",
    "\n",
    "### 1.2 Template Model\n",
    "Language that specifies how ground variables inherit dependencies in the model from a template.  \n",
    "\n",
    "Useful for representing Dynamic Bayesian Networks (Networks that have replication over time).  \n",
    "\n",
    "The following are advantages of using template models:\n",
    "* Template models can often capture events that occur in a time series.\n",
    "* CPDs in template models can often be copied many times.\n",
    "* Template models can capture parameter sharing within a model.\n",
    "* NOTE: Template models cannot represent CPDs that cannot be represented in non-template models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2. Temporal Models - DBNs\n",
    "### 2.1 Distribution over temporal trajectories\n",
    "* discretize time by picking a time granularity: $\\Delta$  \n",
    "* $X^{(t)}$ - a specific instantiation of variable $X$ at time $t\\Delta$\n",
    "* $X^{(t:t')} = \\{X^{(t)}, ..., X^{(t')} \\}$ where $(t \\leq t')$  \n",
    "* Out goal is to be able to represent a probability distribution $P(X^{(t:t')})$ for any $t$, $t'$.   \n",
    "\n",
    "### 2.2 Markov Assumption\n",
    "If we have 1,000,000 timepoints we would need 1,000,000 representations of $X^{(t)}$ to represent this model.  This is not feasible!  \n",
    "\n",
    "The Markov Assumption allows us to compact the model  \n",
    "\n",
    "The chain rule for probabilities gives the following  \n",
    "$$P(X^{(0:T)}) = P(X^{(0)}) \\prod^{T-1}_{t=0} P(X^{(t+1)} \\mid P(X^{(0:t)})$$  \n",
    "\n",
    "This reads as follows:  \n",
    "The set of variables spanning the time from $0,...,T$ equals the probability of X at time 0 multiplied by the state of the system at $t+1$ (i.e., $P(X^{(0)})$) given all previous states at timepoints $0, ..., t$ (i.e.,  $P(X^{(t+1)} \\mid P(X^{(0:t)})$)  \n",
    "\n",
    "The **Markov Assumption**  \n",
    "$$(X^{(t+1)} \\bot X^{(0:t-1)}) \\mid X^{(t)}$$  \n",
    "\n",
    "The state at T+1 is independent of the past given the present.  \n",
    "\n",
    "Now we can simplify the previous statement to:  \n",
    "  \n",
    "$$P(X^{(0:T)}) = P(X^{(0)}) \\prod^{T-1}_{t=0} P(X^{(t+1)} \\mid P(X^{(t)})$$  \n",
    "\n",
    "**this assumption is sometimes too strong**  \n",
    "To address this case, we can enrich the model to include variables that would address the missing information.  For instance, adding the random variable velocity to the model when developing a model for robot positioning.   \n",
    "\n",
    "also, Semi-Markov models.  \n",
    "\n",
    "**Even now we still have a probability distribution for every timepoint at $X^{(t)}$**  \n",
    "\n",
    "This is where we are going to end-up with a template model...  \n",
    "\n",
    "### 2.3 Time Invariance\n",
    "For every $X^{(t)}$ we will have the following **template probability model**: $P(X' \\mid X)$  \n",
    "\n",
    "Important notation in template probability models:  \n",
    "\n",
    "$X^{(t+1)} = X'$ denotes the next timpeoint  \n",
    "$X^{(t)} = X$ denotes the current timepoint  \n",
    "\n",
    "For every single timepoint $t$, this model is replicated such that:  \n",
    "\n",
    "$$P(X^{(t+1)}) \\mid P(X^{(t)}) = P(X' \\mid X)$$  \n",
    "\n",
    "This replication is called **time invariance**  \n",
    "\n",
    "### 2.4 Transition Model  \n",
    "<img src=\"./images/template_transition_model.png\">  \n",
    "<img src=\"./images/initial_state_description.png\">  \n",
    "\n",
    "### 2.5 Two-Time-Slice Bayesian Netowrk (2TBN)  \n",
    "A transition model (2TBN) over $X_1, ..., X_n$ is specified as a BN fragment such that:  \n",
    "\n",
    "* the nodes include $X'_1, ..., X'_n$ and a subset of $X_1, ..., X_n$  \n",
    "* only the nodes $X'_1, ..., X'_n$ have parents and a CPD  \n",
    "\n",
    "The 2TBN defines a conditional distribution:  \n",
    "\n",
    "$$P(X' \\mid X) = \\prod^n_{i=1} P(X'_i \\mid Pa_{x'_i})$$\n",
    "\n",
    "$Pa$ denotes the parents  \n",
    "\n",
    "<img src=\"./images/2tbn.png\">  \n",
    "\n",
    "### 2.6 Dynamic Bayesian Netowrk (DBN)\n",
    "A dynamic Bayesian network (DBN) over $X_1,...,X_n$ is defined by a:  \n",
    "\n",
    "1. 2TBN, defined as $BN_{\\rightarrow}$, over $X_1, ..., X_n$  \n",
    "2. a Bayesian network $BN^{(0)}$ over $X^{(0)}_1,...,X^{(0)}_n$  \n",
    "\n",
    "### 2.7 Ground Baysian Network (Unrolled Network)\n",
    "for a trajectory over $0,...,T$ we define a ground (unrolled) network such that:  \n",
    "\n",
    "1. the dependency model for $X^{(0)}_1,...,X^{(0)}_n$ is copied from $BN^{(0)}$  \n",
    "2. the dependency model for $X^{(t)}_1,...,X^{(t)}_n$ for all $t>0$ is copied from $BN_{\\rightarrow}$  \n",
    "\n",
    "<img src=\"./images/ground_bayesian_network.png\">   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 3. Hidden Markov Models  \n",
    "\n",
    "### 3.1 definition  \n",
    "\n",
    "Hidden Markov Models (HMM) are comprised of a **Transition Model**, and an **Observation Model**  \n",
    "\n",
    "<img src=\"./images/HMM_example.png\"> \n",
    "**(left)** 2TBN HMM template model.  **(Right)** HMM ground network.  \n",
    "\n",
    "There is often a great deal of complexity encoded in the **transition model**.  \n",
    "\n",
    "<img src=\"./images/transition_model.png\"> \n",
    "This is an example CPD of the transition model for the 2TBN of a HMM.  The nodes in this graph represent the four states that the variable $S'$ can take according the the CPD $P(s' \\mid s)$.  Arrows represent the probability of the variable transitioning from the current state $s$ to the next state $s'$.  \n",
    "\n",
    "### 3.2 Applications\n",
    "* Robot localization\n",
    "* Speech Recognition (Best success story)\n",
    "* Biological sequence analysis\n",
    "* Text annotation\n",
    "\n",
    "### 3.3 Examples\n",
    "**Spectrogram of speech signal**  \n",
    "<img src=\"./images/acoustic_signal.png\">  \n",
    "\n",
    "\n",
    "**HMM used to determine the phoneme sequence**  \n",
    "<img src=\"./images/word_HMM.png\">  \n",
    "\n",
    "\n",
    "**HMM used to predict the word from the lexicon baed on phoneme sequences**  \n",
    "<img src=\"./images/recognition_HMM.png\">  \n",
    "\n",
    "### 3.4 Summary\n",
    "\n",
    "* HMMs are a subclass of DBNs  \n",
    "* HMMs seem unstructured at the level of random variables  \n",
    "* HMMs structure typically manifests in sparsity and repeated elements within the transition matrix  \n",
    "* HMMs are used in a wide variety of applications for modeling sequences  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4. Plate Models  \n",
    "\n",
    "### 4.1 Purpose \n",
    "Useful for encoding networks that have **multiple objects of the same type**  \n",
    "\n",
    "### 4.2 Modeling Repetition  \n",
    "A plate denotes $n$ occurrences of a variable $t$.  \n",
    "\n",
    "The plate also contains random variables $X$.  \n",
    "\n",
    "For every instance of the plate variable $t_n = i$, there is a random variable $X(t_i)$  \n",
    "\n",
    "<img src=\"./images/plate_1.png\">  \n",
    "This plate model represents multiple coin tosses.  \n",
    "\n",
    "### 4.3 Nested Plates\n",
    "Plates within a plate are nested plates\n",
    "\n",
    "Each random variable(s) in the plate is indexed by **both** the variable of the plate and the variable of the plate within which it is nested.  \n",
    "\n",
    "<img src=\"./images/nested_plates.png\">  \n",
    "Example of a nested plate. Note that $G$ and $I$ are indexed by both $s$ and $c$. This example states that each student has a \"course specific\" intelligence. This may or may not be an assumption that we would like.\n",
    "\n",
    "### 4.4 Overlapping Plates\n",
    "Plates that are not nested, but overlap.  \n",
    "\n",
    "<img src=\"./images/overlapping_plate.png\">  \n",
    "The assumption here is that difficulty is a property of the course, intelligence is a property of the student, and only grade is a property of both intelligence and difficulty.  \n",
    "\n",
    "### 4.5 explicit Parameter Sharing  \n",
    "We may explicitly write down that certain random variables share parameters of their CPD using the notation $\\theta$.  \n",
    "\n",
    "<img src=\"./images/parameter_sharing.png\">\n",
    "\n",
    "### 4.6 Collective Inference   \n",
    "<img src=\"./images/collective_inference.png\">  \n",
    "\n",
    "### 4.7 Summary  \n",
    "For a **template variable** $A(U_1,...,U_k)$:  \n",
    "* we have a set of **template parents** $B_1(U_1),...,B_m(U_m)$\n",
    "* each index of the template parent $B$ must be a subset of the indices for its child template variable $A$, such that for $B$: $U_i \\subseteq \\{U_1, ..., U_k \\}$  \n",
    "* we cannot have an index in the parent that does not appear in the child, b/c this would result in a child variable that has up to an infinite number of parents\n",
    "* we can define a **template CPD** $P(A \\mid B_1,...,B_m)$\n",
    "* given the above template variable and template parents, for any instantation $u_1,...,u_k$ to $U_1, ..., U_k$ we would have the following exact **ground network**:  \n",
    "$$B(u_1) \\rightarrow A(u_1,...,u_k) \\leftarrow B(u_k)$$  \n",
    "\n",
    "### 4.8 Summary\n",
    "* template for an infinite set of BNs, each induced by a different set of domain objects  \n",
    "* parameters and structure are reused within a BN and across different BNs\n",
    "* Models encode correlations across multiple objects, allowing collective inference\n",
    "* multiple \"languages\", each with different trade-offs in expressive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5. Overview: Structured CPDs\n",
    "\n",
    "### 5.1 Tabular Representations\n",
    "Traditional representation, but become very very large when there are a lot of variables being conditioned on.\n",
    "\n",
    "5 binary variables will result in a tabular CPD with $2^5$ cells.  \n",
    "\n",
    "### 5.2 General CPD\n",
    "* CPD $P(X \\mid Y_1, ..., Y_k)$ specifies a distribution over $X$ for each assignment of $(y_1, ..., y_k)$\n",
    "* Can use any function to specify a factor $\\Phi (X,Y_1,...,Y_k)$ such that:  \n",
    "$$\\sum_x \\Phi (X,Y_1,...,Y_k) = 1.0$$ for all $(y_1, ..., y_k)$\n",
    "\n",
    "### 5.3 examples\n",
    "* Deterministic CPD\n",
    "* Tree-structured CPD\n",
    "* Logistic CPDs and generalizations\n",
    "* Noisy OR / AND\n",
    "* Linear Gaussians and generalizations\n",
    "\n",
    "### 5.4 Context-Specific Independence\n",
    "$$P \\models (X \\bot_c Y \\mid Z,c)$$  \n",
    "\n",
    "where $c$ is an assignment to some set of variables $C$    \n",
    "\n",
    "$P(X,Y \\mid Z,c) = P(X \\mid Z,c)P(Y \\mid Z,c)$  \n",
    "$P(X \\mid Y,Z,c) = P(X \\mid Z,c)$  \n",
    "$P(Y \\mid X,Z,c) = P(Y \\mid Z,c)$  \n",
    "\n",
    "#### example using the OR model\n",
    "\n",
    "<img src=\"./images/or_model.png\">  \n",
    "\n",
    "How to solve this using the rules for context specific independence from above  \n",
    "<img src=\"./images/contect_specific_independence_ex.jpeg\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 6. Tree-Structured CPDs\n",
    "\n",
    "### 6.1 Job Offer Tree CPD\n",
    "\n",
    "<img src=\"./images/tree_structured_cpd.png\">  \n",
    "\n",
    "Certain **context specific independencies** are implied in a tree CPD as follows:  \n",
    "\n",
    "<img src=\"./images/context_specific_indep_in_tree.png\"> \n",
    "\n",
    "### 6.2 Multiplexor CPD\n",
    "\n",
    "In this example, the $Choice$ variable determines if a connection is present in the graph for the other variables $Letter_1$ and $Letter_2$.\n",
    "\n",
    "the $Choice$ variable acts as a **switch** \n",
    "\n",
    "<img src=\"./images/multiplexor_1.png\"> \n",
    "\n",
    "An important conditional independence in this example is that \n",
    "\n",
    "$$(L_1 \\bot L_2 \\mid J,C)$$\n",
    "\n",
    "In this example:  \n",
    "* $Y$ can take on ONLY ONE of the CPDs for $Z_k$.\n",
    "* $A$ takes on a single value $a \\subset set\\{1,2,...,k\\}$\n",
    "* $A$ tells $Y$ which value of $Z_k$ to take on\n",
    "\n",
    "<img src=\"./images/multiplexor_2.png\"> \n",
    "\n",
    "The Probability Distribution specified for $Y$ is the following:  \n",
    "\n",
    "$$P(Y \\mid A,Z_1,...,Z_k) = \\begin{cases} 1, & \\mbox{if } Y=Z_a \\\\ 0, & \\mbox{if otherwise} \\end{cases}$$  \n",
    "\n",
    "In other words, if  $$A = a \\Rightarrow Y = Z_a$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 7. Independence of Causal Influence\n",
    "\n",
    "### 7.1 Noisy OR CPD\n",
    "\n",
    "some cases of exponentially growing CPDs do not lend themselves to the Tree CPD\n",
    "\n",
    "Another option is the Noisy OR CPD\n",
    "\n",
    "<img src=\"./images/noisy_OR_CPD.png\"> \n",
    "\n",
    "Given that $y = 0$, we know that all $Z_i$'s are 0; in particular, $Z_1$ and $Z_2$ are 0, **so that blocks the trail of influence from $X_1$ to $X_2$**. This is a very subtle point - make sure you understand it!\n",
    "\n",
    "What context-specific independencies are induced by a noisy OR CPD?  \n",
    "\n",
    "$(X_1 \\bot_c X_2 \\mid y^0)$  \n",
    "\n",
    "Given that $y = 0$, we know that all $Z_i$'s are 0; in particular, $Z_1$ are 0, so that blocks the trail of influence from $X_1$ to $X_2$.   \n",
    "\n",
    "### 7.2 Independence of Causal Influence\n",
    "The Noisy OR CPD can be generalize to a broader notion of causal influence called: **Independece of Causal Influence**: When we have  bunch of causes for a variable and each of them acts independently to effect the truth of that variable.  \n",
    "\n",
    "<img src=\"./images/indep_causal_inf.png\">   \n",
    "\n",
    "There are no interactions of each separate cause that acts on $Z$.  \n",
    "\n",
    "Example Models:  \n",
    "* Noisy OR - most commonly used  \n",
    "* Noisy AND  \n",
    "* Noisy Max  \n",
    "* Sigmoid CPD  \n",
    "\n",
    "### 7.3 Sigmoid CPD\n",
    "<img src=\"./images/sigmoid_CPD.png\"> \n",
    "\n",
    "$$Z=w_0+\\sum^k_{(i=1)} w_iX_i $$  \n",
    "\n",
    "$w_i$ - weights that indicate the strength of the effect of the influence of a random variable $X_i$ on $Z$  \n",
    "$w_0$ - bias term \n",
    "\n",
    "$Z$ is a continuous number from $-\\infty$ to $+\\infty$  \n",
    "\n",
    "the sum $Z$ is converted to a real probability distribution by passing it through the sigmoid function.  \n",
    "\n",
    "$$sigmoid(z) = \\frac{e^z}{1+e^z}$$  \n",
    "\n",
    "**Behavior of Sigmoid CPD**  \n",
    "\n",
    "<img src=\"./images/behavior_1.png\">\n",
    "\n",
    "The more parent $X_i$'s that are true, the higher the output of the sigmoid function.  \n",
    "\n",
    "Multiplying $z$ by a factor of 10 makes the slope of the sigmoid function more extreme. \n",
    "\n",
    "$P(y^1 \\mid X_1,...,X_k) =w_0+\\sum^k_{(i=1)} w_iX_i$\n",
    "\n",
    "The odds ratio of $Y$ is: $O(\\textbf{x}) = \\frac{P(y^{1}|\\mathbf{x})}{P(y^{0}|\\mathbf{x})}$  \n",
    "\n",
    "It captures the relative likelihood of the two values of $Y$   \n",
    "\n",
    "By what factor does $O(\\textbf{x})$ change if the value of $X_i$ goes from 0 to 1?\n",
    "\n",
    "Solution = $e^{w_i}$  \n",
    "\n",
    "<img src=\"./images/IMG-3224.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 8. Continuous Variables\n",
    "\n",
    "### 8.1 The Normal Distribution  \n",
    "\n",
    "Normal distributions (Gaussian distributions) are parameterized by their **mean and standard deviation**.  \n",
    "\n",
    "In a **linear Gaussian** the parameters are fixed.\n",
    "\n",
    "In a **Conditional Linear Gaussian**, the parameters are conditioned on the value of another variable.  In the following figure, $alpha$ and $T$ depend on the value of another variable, the Door variable.   \n",
    "\n",
    "<img src=\"./images/gaussian_1.PNG\">  \n",
    "\n",
    "### 8.2 Linear Gaussian Model\n",
    "\n",
    "A linear function of the parents $X_i$ and whose variance doesn't depend at all on the values of the parents.\n",
    "\n",
    "$$X \\sim \\mathcal{N}(\\mu,\\,\\sigma^{2})$$\n",
    "\n",
    "Let L and V be the location and velocity of a car. Assume that the CPD on the right is a linear Gaussian. Which of the following statements could possibly be consistent with that CPD?\n",
    "\n",
    "<img src=\"./images/car_model.PNG\">  \n",
    "Answers: \n",
    "* $L(t+1)$ might possibly end up far from its expected position.  \n",
    "\n",
    "* Due to friction, the single most likely value for $L^{(t+1)}$ is $L^{(t)} + 0.9* V^{(t)} \\Delta t$.\n",
    "\n",
    "### 8.3 Conditional Linear Gaussian Model\n",
    "This introduces the idea of one or more discrete parents where the parameters of the linear Gaussian depend on the value of the discrete parents.  In this example $A$ is the discrete parent.  \n",
    "\n",
    "$$X \\sim \\mathcal{N}(w_{a0} + \\sum w_{ai}X_i ;\\ \\sigma^{2}_a)$$  \n",
    "\n",
    "<img src=\"./images/clg.PNG\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
