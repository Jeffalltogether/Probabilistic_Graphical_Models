{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1 Week 3  \n",
    "## Lecture 1. Pairwaise Markov Networks\n",
    "\n",
    "### 1.1 Undirected Graphs\n",
    "\n",
    "<img src=\"./images/pairwise_MN.png\"> \n",
    "\n",
    "Paramererize an undirected graph with **factors** (also affinity functions, compatibility functions, and soft constraints).  \n",
    "\n",
    "They represent the local happiness of a variable to take on a random assignment.  \n",
    "\n",
    "<img src=\"./images/pairwise_MN_factors.png\">  \n",
    "\n",
    "To understand the \"local happiness\" for an entire graph we need to compute the **Factor product**.  \n",
    "\n",
    "However, a factor product such as $\\tilde{P}(A,B,C,D) = \\Phi_{1}(A,B)\\times\\Phi_{2}(B,C)\\times\\Phi_{3}(C,D)\\times\\Phi_{4}(A,D)$ is not a proper probability distribution, because:  \n",
    "\n",
    "* It does not sum to 1 $(\\sum_{a,b,c,d}P(a,b,c,d)\\neq1)$, and  \n",
    "* It is not necessarily between 0 and 1.  \n",
    "\n",
    "### 1.2 Unormalized Measure  \n",
    "\n",
    "$\\tilde{P}(A,B,C,D)$ is an unnormalized measure as indicated by the `~` symbol.  \n",
    "\n",
    "### 1.3 Parition Function\n",
    "\n",
    "The normalizing constant that will make the factor product $\\tilde{P}(X_i,...,X_n)$ sum to $1$  \n",
    "\n",
    "Defined as:  \n",
    "\n",
    "$$Z_{\\Phi} = \\sum_{X_1,...,X_n} \\tilde{P}_{\\Phi}(X_1,...,X_n)$$\n",
    "\n",
    "### 1.4 Potential of the pariwise factor $\\phi_1(A,B)$\n",
    "\n",
    "Consider the pairwise factor $\\phi_1(A,B)$. The potential of $\\phi_1(A,B)$ isproportional to:\n",
    "\n",
    "* The Marginal Probability Distribution is not proportional to \\phi_1(A,B): $P(A,B) \\nsim \\phi(A,B)$  \n",
    "     (remember $P(A,B) = \\sum_{C,D} P(A,B,C,D)$  )   \n",
    "     \n",
    "<img src=\"./images/marginal_v_factor.png\">  \n",
    "\n",
    "There isn't natural mapping between the probability distribution and the factors used to compose them.\n",
    "\n",
    "### 1.5 Pariwise Markov Network\n",
    "\n",
    "An undirected graph whose nodes are $X_1,...,X_n$ and each edge $X_i-X_j$ is associated with a factor (potential) $\\phi_{ij}(X_i,X_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 2. Gibbs Distribution\n",
    "\n",
    "### 2.1 Expressiveness of Markov Models\n",
    "#### Do Markov models express all possible distributions?\n",
    "\n",
    "Can a fully connected Pariwise Markov Network over $X_1, ..., X_n$ where each $X_i$ has $d$ values. How many parameters does the network have?  \n",
    "\n",
    "**Step 1** - determine the number of **edges** in a fully connected Markov Model with the formula n choose k: $\\binom{n}{k}$  \n",
    "* k = 2 because edges only connect 2 nodes.  \n",
    "* The value of the binomial coefficient for non-negative n and k is given explicitly by:  \n",
    "\n",
    "$$_nC_k = \\binom{n}{k} = \\frac{n!}{(n-k)!n!}$$  \n",
    "\n",
    "**Step 2** - Determine the number of parameters per edge: $d^2$  \n",
    "\n",
    "**Step 3** - multiply them to get the expression:  \n",
    "\n",
    "$$\\binom{n}{k} * d^2$$\n",
    "\n",
    "In the case of the fully connected 4 node model: $\\binom{n}{k} * d^2 = \\binom{4}{2}*d^2 = 6*d^2$  \n",
    "\n",
    "**Complexity of the Binomial Distribution**  \n",
    "\n",
    "A more efficient method to compute individual binomial coefficients is given by the formula:    \n",
    "\n",
    "$$\\binom{n}{k} = \\frac{n^\\underline{k}}{k!} = \\frac{n(n-1)(n-2)...(n-(k-1))}{k(k-1)(k-2)...1}  = \\prod^k_{i=1} \\frac{n+1-i}{i}$$  \n",
    "\n",
    "This gives $\\frac{n^k + O(n^{(k-1)})}{k!}$ , which is in $O(n^k)$  \n",
    "\n",
    "Therefore, the **complexity of the fully connected Pariwise Markov Network** is:  \n",
    "\n",
    "$$O(n^2d^2)$$  \n",
    "\n",
    "### Can we represent any probability distributoin using $O(n^2d^2)$?  \n",
    "No.\n",
    "\n",
    "<img src=\"./images/pairwise_distribution.png\">   \n",
    "\n",
    "These 16 entries were generated by 4 pairs of connections with each node represented by a factor of 4 outcomes each.  Therefore, the total number of unique parameters needed to specify this **Pariwise Distribution** is $4*\\binom{4}{2}$ or $4*3 = 12$.  \n",
    "\n",
    "If this were a **Joint Probability Distribution**, the total number of parameters necessary to specify the distrbution would be $2^n - 1$ or 15.  This computation is complexity $O(d^n)$.  \n",
    "\n",
    "Thus, **pairwise factors** simply do not have enough parameters to encompass the space of **joint distributions**.  \n",
    "\n",
    "### Pairwise Markov Newtworks are not sufficiently expressive to capture all probability distributions!\n",
    "\n",
    "### 2.2 General Gibbs Distribution\n",
    "We need to move away from Pairwise edges, where every factor $\\phi$ has a scope of only 2 variables, to better generalize the distributions in a Markov Network\n",
    "\n",
    "A gibbs distribution is parameterized by a set of facrots $\\Phi$ that is composed of 1 or more factors: called General Factors: $\\phi_i(D_i)$\n",
    "\n",
    "This is the definition of a **Gibbs Distribution**:\n",
    "$$\\Phi = \\{ \\phi_1(D_1),...,\\phi_k (D_k)\\}$$  \n",
    "\n",
    "$$\\tilde{P}_{\\Phi}(X_1,...,X_n) = \\prod^k_{i=1} \\phi_i(D_i)$$  \n",
    "\n",
    "$\\tilde{P}$ is not a proper distribution, therefore we must multiply it by the normalizing constant obtained with the **partition function** to get a proper probability distribution:\n",
    "\n",
    "$$Z_{\\Phi} = \\sum_{X_1,...,X_n} \\tilde{P}_{\\Phi}(X_1,...,X_n)$$  \n",
    "\n",
    "Finally we compute the normalized distribution as follows:  \n",
    "$$P_{\\Phi}(X_1,...,X_n) = \\frac{1}{Z_{\\Phi}} \\tilde{P}_{\\Phi} (X_1,...,X_n)$$  \n",
    "\n",
    "### 2.3 Induced Markov Network   \n",
    "\n",
    "Edges indicate direct probabilistic influence between two nodes in a network.\n",
    "\n",
    "Generally, if we have a set of factors $\\Phi = \\{ \\phi_1(D_1),...,\\phi_k (D_k)\\}$.\n",
    "\n",
    "The **Induced Markov Network** $H_{\\Phi}$ has an edge $X_i-X_j$ whenever there exists a factor $\\phi_m \\in \\Phi$ such that $X_i,X_j \\in D_m$.  \n",
    "\n",
    "Two conditions for $X$ and $Y$ to have an undirected edge in the network are:  \n",
    "\n",
    "* If they appear together in some factor $\\phi$, and  \n",
    "* If there exists a factor $\\phi(X, Y)$.  \n",
    "\n",
    "For example...  \n",
    "<img src=\"./images/induced_MN.png\">  \n",
    "\n",
    "### 2.4 Factorization  \n",
    "\n",
    "Is there a set of parametes that will let us represent the distribution P?\n",
    "\n",
    "$P$ factorizes over $H$ if there exists $\\Phi = \\{ \\phi_1(D_1),...,\\phi_k (D_k)\\}$ such that:\n",
    "* $P=P_{\\Phi}$, and    \n",
    "* $H$ is the induced graph for $\\Phi$  \n",
    "\n",
    "These two things are required to encode $P$ over the distribution $H$\n",
    "\n",
    "<img src=\"./images/factorization_induced_MN.png\">  \n",
    "\n",
    "**We cannot read the factorization from the graph**  \n",
    "\n",
    "### 2.5 Flow of Influence  \n",
    "\n",
    "Influence can flow along any active trail, regardless of the form of the factors.  \n",
    "\n",
    "**Active Trails**  In an induced Markov Network a trail $X_i - ... - X_n$ is active given $Z$ if no $X_i$ is in $Z$  \n",
    "\n",
    "What is the difference between an active trail in a Markov Network and an active trail in a Bayesian Network?  \n",
    "\n",
    "They are different in the case where $Z$ is the descendant of a v-structure.  \n",
    "\n",
    "### 2.6 Summary  \n",
    "\n",
    "* Gibbs distribution represents a distribution as a product of factors\n",
    "* Induced Markov network connects every pair of nodes that are in the same factor\n",
    "* Markov network structure doesn't fully specify the factorization of P\n",
    "* Buy active trails depend only on graph structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 3. Conditional Random Fields\n",
    "### 3.1 Used for Task Specific Predictions \n",
    "* $Y \\sim X_1, ..., X_i$\n",
    "* image segmentation\n",
    "* text processing\n",
    "\n",
    "Compared to Naive Bayes Model:\n",
    "**Naive Bayes**\n",
    "* Naive Bayes models each feature as independent\n",
    "* this naive way of thinking does not account for redundant/correlated features\n",
    "* If we have features that are very correlated represented in a Naive Bayes model, then we are ignoring the correlation structure and we can repeatedly count a feature over and over causing skewed results.\n",
    "* adding edges to account for correlations is dificult, because features are dificult to interpret, and it results in very densely connected models.\n",
    "\n",
    "**Conditionl Random Field**\n",
    "* Insead of modeling the joint distribution $P(X,Y)$ we will model the conditional distribution $P(Y \\mid X)$.  \n",
    "* We will *not* try to capture the distribution over $X$, and therefore do not care about the correlations between $X$'s.\n",
    "\n",
    "### 3.2 CRF Representation\n",
    "\n",
    "Just like a Gibbs Distribution, we have a set of factors with their scope\n",
    "$$\\Phi = \\{ \\phi_1(D_1),...,\\phi_k (D_k)\\}$$  \n",
    "\n",
    "Just like Gibbs Distribution we multiply the set of factors to get the unnormalized measure\n",
    "$$\\tilde{P}_{\\Phi}(X,Y) = \\prod^k_{i=1} \\phi_i(D_i)$$  \n",
    "\n",
    "If we want to model the conditional distribution of Y given X, we need to have X on the right hand side of the conditioning bar and we need a separate normalization constant for each X as:\n",
    "\n",
    "$$Z_{\\Phi}(X) = \\sum_{Y} \\tilde{P}_{\\Phi}(X,Y)$$\n",
    "\n",
    "Finally we compute the normalized distribution as follows:  \n",
    "$$P_{\\Phi}(Y \\mid X) = \\frac{1}{Z_{\\Phi}(X)} \\tilde{P}_{\\Phi} (X,Y)$$  \n",
    "\n",
    "This defines a family of conditional distributions that vary by X:  \n",
    "\n",
    "<img src=\"./images/family_CPD.png\">  \n",
    "\n",
    "Consider a new factor $\\phi(\\mathbf{D})$ such that $\\mathbf{D} \\subseteq X$.\n",
    "\n",
    "What is the effect of adding this factor $\\phi$ on the distribution $P(Y\\mid X)$?\n",
    "\n",
    "Answer: $\\phi$ multiplies both the unnormalized measure and the partition function and therefore cancels out, so $P(Y\\mid X)$ remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pgmpy.factors.discrete import DiscreteFactor as Factor\n",
    "from pgmpy.models import MarkovModel\n",
    "G = MarkovModel([('Y','X1'),('Y','X2')])\n",
    "\n",
    "factor_X1 = Factor(['Y','X1'],\n",
    "                  [2,2],\n",
    "                  [5,10,20,15])\n",
    "\n",
    "factor_X2 = Factor(['Y','X2'],\n",
    "                  [2,2],\n",
    "                  [3,4,1,2])\n",
    "\n",
    "G.add_factors(factor_X1,factor_X2)\n",
    "#G.get_factors()\n",
    "G.get_local_independencies()\n",
    "G.get_partition_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+----------------+\n",
      "| Y   | X1   | X2   |   phi(Y,X1,X2) |\n",
      "|-----+------+------+----------------|\n",
      "| Y_0 | X1_0 | X2_0 |        15.0000 |\n",
      "| Y_0 | X1_0 | X2_1 |        20.0000 |\n",
      "| Y_0 | X1_1 | X2_0 |        30.0000 |\n",
      "| Y_0 | X1_1 | X2_1 |        40.0000 |\n",
      "| Y_1 | X1_0 | X2_0 |        20.0000 |\n",
      "| Y_1 | X1_0 | X2_1 |        40.0000 |\n",
      "| Y_1 | X1_1 | X2_0 |        15.0000 |\n",
      "| Y_1 | X1_1 | X2_1 |        30.0000 |\n",
      "+-----+------+------+----------------+\n"
     ]
    }
   ],
   "source": [
    "Psi = factor_X1 * factor_X2\n",
    "print Psi\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
